{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def res_to_int(res_label):\n",
    "    res_dict = {\n",
    "        'GUA': 1,\n",
    "        'CYT': 2,\n",
    "        'ADE': 3,\n",
    "        'URA': 4\n",
    "    }\n",
    "    for i in range(len(res_label)):\n",
    "        res_label[i] = res_dict[res_label[i]]\n",
    "    return np.array(res_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       2es5_c4  45  7  14  45.1  45.2  135  132  196  390  9\n",
      "0      1d0u_c2  46  7  14    46    46  138  135  200  405  9\n",
      "1      1ync_c2  46  7  14    46    46  138  135  200  410  9\n",
      "2      2ixz_c1  43  7  14    43    43  129  126  188  365  9\n",
      "3      1jtj_c4  45  7  14    45    45  135  132  196  395  9\n",
      "4      1txs_c8  42  7  14    42    42  126  123  184  355  9\n",
      "...        ...  .. ..  ..   ...   ...  ...  ...  ...  ... ..\n",
      "1653   1hwq_c8  47  7  14    47    47  141  138  204  415  9\n",
      "1654  1p5m_c15  45  7  14    45    45  135  132  196  390  9\n",
      "1655   1r7z_c1  45  7  14    45    45  135  132  196  390  9\n",
      "1656   1r7z_c8  46  7  14    46    46  138  135  200  410  9\n",
      "1657   1p5m_c5  46  7  14    46    46  138  135  200  400  9\n",
      "\n",
      "[1658 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# handle csv files\n",
    "\n",
    "file = 'data/SeqCSV/seq_frame.csv'\n",
    "df = pd.read_csv(file)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df.iloc[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file = 'data/SeqCSV/2es5_c4.csv'\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2.0000,   1.0000,   1.0000,  ...,   6.0000,   6.0000,  13.8717],\n",
      "        [  1.0000,   1.0000,   6.0000,  ...,   9.0000,   9.0000,  40.7660],\n",
      "        [  4.0000,   3.0000,   7.0000,  ...,  12.0000,  12.0000, 181.4625],\n",
      "        ...,\n",
      "        [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "        [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "        [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],\n",
      "       dtype=torch.float64)\n",
      "[ 2  1  4  5  6  7  3  2  1  4  5  6  7  3  2  1  4  5 11  3  2  1  4  5\n",
      "  8  9  3  2  1  4  5  6  7  3  2  1  4  5 11  3  2  1  4  5 10]\n"
     ]
    }
   ],
   "source": [
    "feat = np.array(df)\n",
    "feat = torch.from_numpy(feat)\n",
    "print(feat)\n",
    "atoms = np.array(feat[0:45,0], dtype=int)\n",
    "print(atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "coords = torch.tensor(feat[0:135,5], dtype=torch.float).reshape(-1,3)\n",
    "print(len(coords.shape))\n",
    "coords = torch.tensor(feat[0:135,5], dtype=torch.float).reshape(-1,)\n",
    "print(len(coords.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[                   6                   18                    1]\n",
      " [                   6                    9                    2]\n",
      " [                   9                   12                    3]\n",
      " [                  12                   15                    7]\n",
      " [                   3                    6                    9]\n",
      " [                   0                    3                   11]\n",
      " [                  27                   39                    1]\n",
      " [                  27                   30                    2]\n",
      " [                  30                   33                    3]\n",
      " [                  33                   36                    7]\n",
      " [                  24                   27                    9]\n",
      " [                  18                   21                   10]\n",
      " [                  21                   24                   11]\n",
      " [                  48                   57                    1]\n",
      " [                  48                   51                    2]\n",
      " [                  51                   54                    6]\n",
      " [                  45                   48                    9]\n",
      " [                  39                   42                   10]\n",
      " [                  42                   45                   11]\n",
      " [                  66                   78                    1]\n",
      " [                  66                   69                    2]\n",
      " [                  69                   72                    4]\n",
      " [                  72                   75                    8]\n",
      " [                  63                   66                    9]\n",
      " [                  57                   60                   10]\n",
      " [                  60                   63                   11]\n",
      " [                  87                   99                    1]\n",
      " [                  87                   90                    2]\n",
      " [                  90                   93                    3]\n",
      " [                  93                   96                    7]\n",
      " [                  84                   87                    9]\n",
      " [                  78                   81                   10]\n",
      " [                  81                   84                   11]\n",
      " [                 108                  117                    1]\n",
      " [                 108                  111                    2]\n",
      " [                 111                  114                    6]\n",
      " [                 105                  108                    9]\n",
      " [                  99                  102                   10]\n",
      " [                 102                  105                   11]\n",
      " [                 126                  129                    2]\n",
      " [                 129                  132                    5]\n",
      " [                 123                  126                    9]\n",
      " [                 117                  120                   10]\n",
      " [                 120                  123                   11]\n",
      " [-9223372036854775808 -9223372036854775808 -9223372036854775808]]\n"
     ]
    }
   ],
   "source": [
    "bonds = np.array(feat[0:135,6],dtype=int).reshape(-1,3)\n",
    "print(bonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_target(X):\n",
    "    if len(X.shape) == 2:\n",
    "        X = X.unsqueeze(0)\n",
    "    print(X.shape)\n",
    "    print(X[:,0:3,9])\n",
    "    target = torch.sum(X[:,0:3,9],dim=1).squeeze()\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 445, 10])\n",
      "tensor([[ 13.8717,  40.7660, 181.4625]], dtype=torch.float64)\n",
      "tensor(236.1002, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "res = get_target(feat)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2.0000,   1.0000,   1.0000,  ...,   6.0000,   6.0000,  13.8717],\n",
       "        [  1.0000,   1.0000,   6.0000,  ...,   9.0000,   9.0000,  40.7660],\n",
       "        [  4.0000,   3.0000,   7.0000,  ...,  12.0000,  12.0000, 181.4625],\n",
       "        ...,\n",
       "        [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1d0u_c2\n"
     ]
    }
   ],
   "source": [
    "frame = 'data/SeqCSV/seq_frame.csv'\n",
    "df = pd.read_csv(frame)\n",
    "name = df.iloc[0,0]\n",
    "print(name)\n",
    "lengths = np.array(df.iloc[0,1:].astype('int64'))\n",
    "lengths = torch.from_numpy(lengths)\n",
    "\n",
    "csvfile = 'data/SeqCSV/'+name+'.csv'\n",
    "df = pd.read_csv(csvfile)\n",
    "features = np.array(df)\n",
    "features = torch.from_numpy(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 46,   7,  14,  46,  46, 138, 135, 200, 405,   9])\n"
     ]
    }
   ],
   "source": [
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms = features[:lengths[0],0].long()\n",
    "coords = features[:lengths[5],5].reshape(-1,3)\n",
    "bonds = features[:lengths[6],6].long().reshape(-1,3)\n",
    "angles = features[:lengths[7],7].long().reshape(-1,4)\n",
    "tors = features[:lengths[8],8].long().reshape(-1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def is_float(n):\n",
    "    try:\n",
    "        float(n)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def load_pars():\n",
    "    rootpath = '../Structure_DB_Amber_HiRE'\n",
    "\n",
    "    with open(rootpath + '/Prep_pureHire/Output/1ato/' + 'parameters.top', 'r') as f:\n",
    "        reader = f.read()\n",
    "\n",
    "        text = reader.split(\"SECTION BOND_FORCE_CONSTANT\")[1].split(\"SECTION ANGLE_FORCE_CONSTANT\")[0].strip()\n",
    "        bond_type = np.array([float(i) for i in text.split() if is_float(i)], dtype='float64').reshape(2, -1).transpose()\n",
    "\n",
    "        text = reader.split(\"SECTION ANGLE_FORCE_CONSTANT\")[1].split(\"SECTION DIHEDRAL_FORCE_CONSTANT\")[0].strip()\n",
    "        angle_type = np.array([float(i) for i in text.split() if is_float(i)], dtype='float64').reshape(2, -1).transpose()\n",
    "\n",
    "        text = reader.split(\"SECTION DIHEDRAL_FORCE_CONSTANT\")[1].split(\"SECTION BONDS\")[0].strip()\n",
    "        torsion_type = np.array([float(i) for i in text.split() if is_float(i)], dtype='float64').reshape(3, -1).transpose()\n",
    "\n",
    "    fixed_pars = {\n",
    "        'bond_type': bond_type,\n",
    "        'angle_type': angle_type,\n",
    "        'torsion_type': torsion_type\n",
    "    }\n",
    "\n",
    "    with open(rootpath + '/Prep_pureHire/scale_RNA.dat', 'r') as f:\n",
    "        pars = []\n",
    "        for line in f.readlines():\n",
    "            pars.append(float(line.split()[1]))\n",
    "        pars = np.array(pars, dtype='float64')\n",
    "\n",
    "    pickle.dump(fixed_pars, open('data/SeqCSV/fixed_pars.p', 'wb'))\n",
    "    pickle.dump(pars, open('data/SeqCSV/pars.p', 'wb'))\n",
    "    print('parameters saved')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters saved\n"
     ]
    }
   ],
   "source": [
    "load_pars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 30.0000,   3.8000],\n",
      "        [200.0000,   2.3440],\n",
      "        [200.0000,   2.6220],\n",
      "        [200.0000,   2.6330],\n",
      "        [200.0000,   3.0620],\n",
      "        [200.0000,   3.0040],\n",
      "        [200.0000,   2.4500],\n",
      "        [200.0000,   2.1800],\n",
      "        [200.0000,   1.5200],\n",
      "        [200.0000,   1.5930],\n",
      "        [200.0000,   1.4300],\n",
      "        [ 40.0000,  12.0000],\n",
      "        [ 10.0000,  12.0000],\n",
      "        [ 10.0000,  12.0000],\n",
      "        [ 10.0000,  12.0000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "fixed_pars = pickle.load(open('data/SeqCSV/fixed_pars.p', 'rb'))\n",
    "opt_pars = pickle.load(open('data/SeqCSV/pars.p', 'rb'))\n",
    "bt = torch.from_numpy(fixed_pars['bond_type'])\n",
    "at = torch.from_numpy(fixed_pars['angle_type'])\n",
    "tt = torch.from_numpy(fixed_pars['torsion_type'])\n",
    "pars = torch.from_numpy(opt_pars)\n",
    "print(bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.5153, dtype=torch.float64)\n",
      "0.0034055709838867188\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "a1 = (bonds[:,0]/3).long()\n",
    "a2 = (bonds[:,1]/3).long()\n",
    "idx = bonds[:,2]-1\n",
    "d = torch.linalg.norm(coords[a1,:]-coords[a2,:],dim=1)\n",
    "e = pars[0]*bt[idx,0]*(d-bt[idx,1])**2\n",
    "print(e.sum())\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonds_energy(coords, bonds, bond_type, pars):\n",
    "    \"\"\"\n",
    "    :param coords: 3*Natoms 2-array with xyz coordinates\n",
    "    :param bonds: 3-array with 3*Nbonds elements: atom1, atom2, bond_type index\n",
    "    :param bond_type: 2-array with bond_strength, bond_eq for Nbonds\n",
    "    :param pars: array of parameters to optimize (only first one eneters calculation)\n",
    "    :return: energy associated with covalent bonds\n",
    "    \"\"\"\n",
    "    a1 = bonds[:, 0] / 3  # gives index of the 1st atom for the coordinates\n",
    "    a2 = bonds[:, 1] / 3  # gives index of the 2nd atom for the coordinates\n",
    "    bt = bonds[:, 2] - 1    # gives index of bond type\n",
    "    n_bonds = bonds.shape[0]\n",
    "    e_bonds = torch.tensor(0.0)\n",
    "\n",
    "    for n in range(n_bonds):\n",
    "        i = int(a1[n])\n",
    "        j = int(a2[n])\n",
    "        ind = int(bt[n])\n",
    "        d = coords[i, :] - coords[j, :]\n",
    "        d = (torch.dot(d, d))**0.5   # distance between atoms\n",
    "        e_bonds += bond_type[ind, 0] * (d-bond_type[ind, 1])**2\n",
    "    e_bonds *= pars[0]\n",
    "    return e_bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.5153)\n",
      "0.007597446441650391\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "e2 = bonds_energy(coords,bonds,bt,pars)\n",
    "print(e2)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_thetatype(atoms,at1,at2,at3):\n",
    "    thty = torch.zeros(len(at1)).long()\n",
    "\n",
    "    a1 = atoms[at1]\n",
    "    a2 = atoms[at2]\n",
    "    a3 = atoms[at3]\n",
    "    \n",
    "    thty[(a1==1) & (a3==3)] = 4\n",
    "    thty[(a1==1) & (a3==5)] = 6\n",
    "    thty[(a1==2) & (a2==1)] = 3\n",
    "    thty[(a1==3) & (a2==2)] = 2\n",
    "    # thty[(a1==4) & (a2==5)] = 0\n",
    "    thty[(a1==4) & (a2==3)] = 5\n",
    "    thty[(a1==5) & ((a2==6) | (a2==8))] = 1\n",
    "    thty[(a1==5) & (a2==4)] = 7\n",
    "    \n",
    "    return thty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(517.7064, dtype=torch.float64)\n",
      "0.15335960182496888\n",
      "0.0022993087768554688\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "a1 = (angles[:,0]/3).long()\n",
    "a2 = (angles[:,1]/3).long()\n",
    "a3 = (angles[:,2]/3).long()\n",
    "idx = angles[:,3]-1\n",
    "\n",
    "t1 = time.time()\n",
    "thetatype = assign_thetatype(atoms,a1,a2,a3)\n",
    "t2 = time.time()\n",
    "\n",
    "rij = coords[a1,:] - coords[a2,:]\n",
    "rkj = coords[a3, :] - coords[a2, :]\n",
    "an = torch.arccos((rij*rkj).sum(dim=1)/(torch.linalg.norm(rij,dim=1)*torch.linalg.norm(rkj,dim=1)))\n",
    "e = (pars[46]*pars[thetatype+1]*at[idx, 0]*(an - at[idx, 1])**2).sum()\n",
    "tf = time.time()\n",
    "\n",
    "print(e)\n",
    "print((t2-t1)/(tf-t0))\n",
    "print(tf-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_theta(atoms, i1, i2, i3):\n",
    "    a1 = atoms[i1]\n",
    "    a2 = atoms[i2]\n",
    "    a3 = atoms[i3]\n",
    "    k_type = 0\n",
    "    if a1 == 1:\n",
    "        if a3 == 3:\n",
    "            k_type = 4\n",
    "        elif a3 == 5:\n",
    "            k_type = 6\n",
    "    elif a1 == 2:\n",
    "        if a2 == 1:\n",
    "            k_type = 3\n",
    "    elif a1 == 3:\n",
    "        if a2 == 2:\n",
    "            k_type = 2\n",
    "    elif a1 == 4:\n",
    "        if a2 == 5:\n",
    "            k_type = 0\n",
    "        elif a2 == 3:\n",
    "            k_type = 5\n",
    "    elif a1 == 5:\n",
    "        if a2 == 6:\n",
    "            k_type = 1\n",
    "        elif a2 == 8:\n",
    "            k_type = 1\n",
    "        elif a2 == 4:\n",
    "            k_type = 7\n",
    "    else:\n",
    "        print(\"ERROR: Unknown angle type\")\n",
    "    return k_type\n",
    "\n",
    "\n",
    "def angles_energy(atoms, coords, angles, angle_type, pars):\n",
    "    \"\"\"\n",
    "    :param atoms: Natoms 1-array with atom name\n",
    "    :param coords: 3*Natoms 2-array with xyz coordinates\n",
    "    :param angles: 4-array with 4*Nangles elements: atom1, atom2, atom 3, angle_type index\n",
    "    :param angle_type: 2-array with angle_strength, angle_eq for Nangles\n",
    "    :param pars: array of parameters to optimize (only second one eneters calculation)\n",
    "    :return: energy associated with angles\n",
    "    \"\"\"\n",
    "    a1 = angles[:, 0] / 3  # gives index of the 1st atom for the coordinates\n",
    "    a2 = angles[:, 1] / 3  # gives index of the 2nd atom for the coordinates\n",
    "    a3 = angles[:, 2] / 3  # gives index of the 2nd atom for the coordinates\n",
    "    at = angles[:, 3] - 1  # gives index of angle type\n",
    "    n_angles = angles.shape[0]\n",
    "    e_angles = torch.tensor(0.0)\n",
    "\n",
    "    for n in range(n_angles):\n",
    "        i = int(a1[n])\n",
    "        j = int(a2[n])\n",
    "        k = int(a3[n])\n",
    "        ind = int(at[n])\n",
    "        rij = coords[i, :] - coords[j, :]\n",
    "        rkj = coords[k, :] - coords[j, :]\n",
    "        c_an = torch.dot(rij, rkj)/(torch.norm(rij)*torch.norm(rkj))  # cos_theta\n",
    "\n",
    "        # regularization\n",
    "        # eps = torch.tensor(0.001)\n",
    "        # c_an = min(max(c_an, -1+eps), 1-eps)\n",
    "\n",
    "        an = torch.arccos(c_an)\n",
    "\n",
    "        # define angle_type multiplier index depending on atoms involved\n",
    "        thetatype = assign_theta(atoms, i, j, k)\n",
    "        e_angles += pars[46] * pars[thetatype+1] * angle_type[ind, 0] * (an - angle_type[ind, 1])**2\n",
    "    return e_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(517.7064)\n",
      "0.012653350830078125\n"
     ]
    }
   ],
   "source": [
    "angle_type = at\n",
    "t0 = time.time()\n",
    "e2 = angles_energy(atoms,coords,angles,angle_type,pars)\n",
    "print(e2)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_phitype(atoms,at1,at2,at3,at4):\n",
    "    phty = torch.zeros(len(at1)).long()\n",
    "    a1 = atoms[at1]\n",
    "    a2 = atoms[at2]\n",
    "    a3 = atoms[at3]\n",
    "    a4 = atoms[at4]\n",
    "    \n",
    "    phty[(a1==1) & (a3==5)] = 2\n",
    "    phty[(a1==1) & (a3==3)] = 4\n",
    "    phty[(a1==2) & (a4==3)] = 6\n",
    "    phty[(a1==2) & (a4==5)] = 7\n",
    "    phty[(a1==3) & (a2==4)] = 3\n",
    "    phty[(a1==3) & (a2==2)] = 8\n",
    "    phty[(a1==4) & (a2==5)] = 0 \n",
    "    phty[(a1==4) & ((a2==6) | (a2==8))] = 1\n",
    "    phty[(a1==4) & (a2==3)] = 9\n",
    "    phty[(a1==5) & (a2==4)] = 5\n",
    "      \n",
    "    \n",
    "    return phty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_phi(atoms,i1,i2,i3,i4):\n",
    "    a1 = atoms[i1]\n",
    "    a2 = atoms[i2]\n",
    "    a3 = atoms[i3]\n",
    "    a4 = atoms[i4]\n",
    "    ktype = 0\n",
    "    if a1 == 1:\n",
    "        if a3 == 5:\n",
    "            ktype = 2\n",
    "        elif a3 == 3:\n",
    "            ktype = 4\n",
    "    elif a1 == 2:\n",
    "        if a4 == 3:\n",
    "            ktype = 6\n",
    "        elif a4 == 5:\n",
    "            ktype = 7\n",
    "    elif a1 == 3:\n",
    "        if a2 == 4:\n",
    "            ktype = 3\n",
    "        elif a2 == 2:\n",
    "            ktype = 8\n",
    "    elif a1 == 4:\n",
    "        if a2 == 5:\n",
    "            ktype = 0\n",
    "        elif a2 == 6 or a2 == 8:\n",
    "            ktype = 1\n",
    "        elif a2 == 3:\n",
    "            ktype = 9\n",
    "    elif a1 == 5:\n",
    "        if a2 == 4:\n",
    "            ktype = 5\n",
    "    else:\n",
    "        print(\"ERROR: Unknown dihedral type\")\n",
    "    return ktype\n",
    "\n",
    "\n",
    "def torsions_energy(atoms, coords, tors, tors_type, pars):\n",
    "    \"\"\"\n",
    "    :param atoms: Natoms 1-array with atom name\n",
    "    :param coords: 3*Natoms 2-array with xyz coordinates\n",
    "    :param tors: 5-array with 5*Ntors elements: atom1, atom2, atom 3, atom 4, tors_type index\n",
    "    :param tors_type: 3-array with tors_strength, tors_eq, tors_phase for Ntors\n",
    "    :param pars: array of parameters to optimize\n",
    "    :return: energy associated with torsions\n",
    "    \"\"\"\n",
    "    a1 = tors[:, 0] / 3  # gives index of the 1st atom for the coordinates\n",
    "    a2 = tors[:, 1] / 3  # gives index of the 2nd atom for the coordinates\n",
    "    a3 = tors[:, 2] / 3  # gives index of the 2nd atom for the coordinates\n",
    "    a4 = tors[:, 3] / 3  # gives index of the 2nd atom for the coordinates\n",
    "    tt = tors[:, 4] - 1  # gives index of tors type\n",
    "    n_tors = tors.shape[0]\n",
    "    e_tors = torch.tensor(0.0)\n",
    "\n",
    "    for n in range(n_tors):\n",
    "        i = int(a1[n])\n",
    "        j = int(a2[n])\n",
    "        k = int(a3[n])\n",
    "        l = int(a4[n])\n",
    "        ind = int(tt[n])\n",
    "\n",
    "        rij = coords[i, :] - coords[j, :]\n",
    "        rkj = coords[k, :] - coords[j, :]\n",
    "        rkl = coords[k, :] - coords[l, :]\n",
    "        nj = torch.cross(rij, rkj)\n",
    "        nj = nj / torch.norm(nj)\n",
    "        nk = torch.cross(rkl, rkj)\n",
    "        nk = nk / torch.norm(nk)\n",
    "        phi = torch.acos(torch.dot(nj, nk))\n",
    "        phi = torch.tensor(math.pi) - phi*torch.sign(torch.dot(rkj, torch.cross(nk, nj)))\n",
    "\n",
    "        phitype = assign_phi(atoms, i, j, k, l)\n",
    "        e_tors += pars[9] * pars[28+phitype] * tors_type[ind, 0] * (1 + torch.cos(tors_type[ind, 1]*phi - tors_type[ind, 2]))\n",
    "\n",
    "    return e_tors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(55.1384, dtype=torch.float64)\n",
      "0.2289292777097655\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "t0 = time.time()\n",
    "a1 = (tors[:,0]/3).long()\n",
    "a2 = (tors[:,1]/3).long()\n",
    "a3 = (tors[:,2]/3).long()\n",
    "a4 = (tors[:,3]/3).long()\n",
    "idx = tors[:,4]-1\n",
    "\n",
    "t1 = time.time()\n",
    "phitype = assign_phitype(atoms,a1,a2,a3,a4)\n",
    "t2 = time.time()\n",
    "\n",
    "rij = coords[a1,:] - coords[a2,:]\n",
    "rkj = coords[a3, :] - coords[a2, :]\n",
    "rkl = coords[a3, :] - coords[a4, :]\n",
    "nj = torch.linalg.cross(rij,rkj,dim=1)\n",
    "nk = torch.linalg.cross(rkl,rkj,dim=1)\n",
    "phi = torch.acos((nj*nk).sum(dim=1)/(torch.linalg.norm(nj,dim=1)*torch.linalg.norm(nk,dim=1)))\n",
    "phi = torch.tensor(math.pi) - phi*torch.sign((rkj*torch.linalg.cross(nk,nj,dim=1)).sum(dim=1))\n",
    "e = (pars[9]*pars[28+phitype]*tt[idx, 0]*(1+torch.cos(tt[idx, 1]*phi - tt[idx, 2]))).sum()\n",
    "tf = time.time()\n",
    "\n",
    "print(e)\n",
    "print((t2-t1)/(tf-t0))\n",
    "print()\n",
    "#nj = nj/torch.linalg.norm(nj,dim=1)\n",
    "#print(nj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(55.1384, dtype=torch.float64)\n",
      "0.0032567977905273438\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "t0 = time.time()\n",
    "a1 = (tors[:,0]/3).long()\n",
    "a2 = (tors[:,1]/3).long()\n",
    "a3 = (tors[:,2]/3).long()\n",
    "a4 = (tors[:,3]/3).long()\n",
    "idx = tors[:,4]-1\n",
    "\n",
    "phitype = assign_phitype(atoms,a1,a2,a3,a4)\n",
    "rij = coords[a1,:] - coords[a2,:]\n",
    "rkj = coords[a3, :] - coords[a2, :]\n",
    "rkl = coords[a3, :] - coords[a4, :]\n",
    "nj = torch.linalg.cross(rij,rkj,dim=1)\n",
    "nj = nj / torch.linalg.norm(nj,dim=1).unsqueeze(1).expand(-1,3)\n",
    "nk = torch.linalg.cross(rkl,rkj,dim=1)\n",
    "nk = nk / torch.linalg.norm(nk,dim=1).unsqueeze(1).expand(-1,3)\n",
    "phi = torch.acos((nj*nk).sum(dim=1))\n",
    "phi = torch.tensor(math.pi) - phi*torch.sign((rkj*torch.linalg.cross(nk,nj,dim=1)).sum(dim=1))\n",
    "e = (pars[9]*pars[28+phitype]*tt[idx, 0]*(1+torch.cos(tt[idx, 1]*phi - tt[idx, 2]))).sum()\n",
    "print(e)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(55.1384)\n",
      "0.0313112735748291\n"
     ]
    }
   ],
   "source": [
    "torsion_type = tt\n",
    "t0 = time.time()\n",
    "e2 = torsions_energy(atoms,coords,tors,torsion_type,pars)\n",
    "print(e2)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_local(atoms, coords, bonds, bond_type, angles, angle_type, tors, tors_type, pars):\n",
    "    E_bonds = bonds_energy(coords, bonds, bond_type, pars)\n",
    "    E_angles = angles_energy(atoms, coords, angles, angle_type, pars)\n",
    "    E_tors = torsions_energy(atoms, coords, tors, tors_type, pars)\n",
    "    return E_bonds + E_angles + E_tors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(580.3600)\n",
      "0.021758079528808594\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "e2 = E_local(atoms, coords, bonds, bt, angles, angle_type, tors, torsion_type, pars)\n",
    "print(e2)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(580.3600, dtype=torch.float64)\n",
      "0.009308099746704102\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "a1 = (bonds[:,0]/3).long()\n",
    "a2 = (bonds[:,1]/3).long()\n",
    "idx = bonds[:,2]-1\n",
    "d = torch.linalg.norm(coords[a1,:]-coords[a2,:],dim=1)\n",
    "e_bonds = (pars[0]*bt[idx,0]*(d-bt[idx,1])**2).sum()\n",
    "\n",
    "a1 = (angles[:,0]/3).long()\n",
    "a2 = (angles[:,1]/3).long()\n",
    "a3 = (angles[:,2]/3).long()\n",
    "idx = angles[:,3]-1\n",
    "thetatype = assign_thetatype(atoms,a1,a2,a3)\n",
    "rij = coords[a1,:] - coords[a2,:]\n",
    "rkj = coords[a3, :] - coords[a2, :]\n",
    "an = torch.arccos((rij*rkj).sum(dim=1)/(torch.linalg.norm(rij,dim=1)*torch.linalg.norm(rkj,dim=1)))\n",
    "e_angles = (pars[46]*pars[thetatype+1]*at[idx, 0]*(an - at[idx, 1])**2).sum()\n",
    "\n",
    "a1 = (tors[:,0]/3).long()\n",
    "a2 = (tors[:,1]/3).long()\n",
    "a3 = (tors[:,2]/3).long()\n",
    "a4 = (tors[:,3]/3).long()\n",
    "idx = tors[:,4]-1\n",
    "phitype = assign_phitype(atoms,a1,a2,a3,a4)\n",
    "rij = coords[a1,:] - coords[a2,:]\n",
    "rkj = coords[a3, :] - coords[a2, :]\n",
    "rkl = coords[a3, :] - coords[a4, :]\n",
    "nj = torch.linalg.cross(rij,rkj,dim=1)\n",
    "nk = torch.linalg.cross(rkl,rkj,dim=1)\n",
    "phi = torch.acos((nj*nk).sum(dim=1)/(torch.linalg.norm(nj,dim=1)*torch.linalg.norm(nk,dim=1)))\n",
    "phi = torch.tensor(math.pi) - phi*torch.sign((rkj*torch.linalg.cross(nk,nj,dim=1)).sum(dim=1))\n",
    "e_tors = (pars[9]*pars[28+phitype]*tt[idx, 0]*(1+torch.cos(tt[idx, 1]*phi - tt[idx, 2]))).sum()\n",
    "\n",
    "e = e_bonds + e_angles + e_tors\n",
    "print(e)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 3,  5, 10, 12, 17, 19, 25, 27, 33, 35, 41, 43, 49]),) (tensor([ 5, 12, 19, 27, 35, 43, 49]),)\n",
      "tensor([0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
      "        0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
      "        0, 2])\n"
     ]
    }
   ],
   "source": [
    "pt = torch.zeros(len(a1)).long()\n",
    "at1 = atoms[a1]\n",
    "at2 = atoms[a2]\n",
    "at3 = atoms[a3]\n",
    "at4 = atoms[a4]\n",
    "x = (at1 == 1).nonzero(as_tuple=True) \n",
    "y = (at3 == 5).nonzero(as_tuple=True)\n",
    "print(x,y)\n",
    "pt[(at1 == 1) & (at3 == 5)] = 2\n",
    "print(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn import Parameter\n",
    "from numba import cuda\n",
    "\n",
    "class RNASeqDataset(Dataset):\n",
    "    \"\"\"RNA sequences dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, device, csv_file='data/SeqCSV/seq_frame.csv', root_dir='data/SeqCSV/', transform=None):\n",
    "        self.seq_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        # self.transform = transform\n",
    "        size = len(self.seq_frame)\n",
    "        lengths = self.seq_frame.iloc[:, 1:].astype('int64')\n",
    "        lengths = torch.from_numpy(np.array(lengths)).to(device)\n",
    "\n",
    "        # get features size\n",
    "        seq_name = os.path.join(self.root_dir, self.seq_frame.iloc[0, 0] + '.csv')\n",
    "        features = pd.read_csv(seq_name)\n",
    "        row, col = np.array(features).shape\n",
    "\n",
    "        features = torch.zeros(size,row,col)\n",
    "        for i in range(size):\n",
    "            seq_name = os.path.join(self.root_dir, self.seq_frame.iloc[i, 0] + '.csv')\n",
    "            seq = pd.read_csv(seq_name)\n",
    "            features[i,:,:] = torch.from_numpy(np.array(seq))\n",
    "        features = features.to(device)\n",
    "        self.dataset = {'lengths': lengths, 'features': features}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        lengths = self.dataset['lengths'][idx]\n",
    "        features = self.dataset['features'][idx]\n",
    "        sample = {'lengths': lengths, 'features': features}\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import LocalEnergyVct as le\n",
    "\n",
    "def get_target(X):\n",
    "    if len(X['features'].shape) == 2:\n",
    "        X['features'] = X['features'].unsqueeze(0)\n",
    "    # print(torch.sum(X['features'][:,0:3,9],dim=1))\n",
    "    target = (X['features'][:,0:3,9]).to(device)  # /X['features'].shape[0]).squeeze().to(device)\n",
    "    return target\n",
    "\n",
    "\n",
    "def loss_fn(energy,target,lc=1):\n",
    "    batch_size = energy.shape[0]\n",
    "    # jac = torch.autograd.functional.jacobian(energy, pars, create_graph=True)\n",
    "    loss = (energy - target).pow(2).sum() / batch_size  #  + lc * grad.pow(2).sum())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    # size = len(dataloader.dataset)\n",
    "    # num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    num_batches = 0\n",
    "    train_loss = 0\n",
    "\n",
    "    for X in dataloader:\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        target = get_target(X)\n",
    "        loss = loss_fn(pred, target)\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "        num_batches += 1\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= num_batches\n",
    "    print(f'Avg train_loss = {train_loss:>0.4f}, valid batches = {num_batches}')\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalEnergyOpt(nn.Module):\n",
    "\n",
    "    def __init__(self,fixed_pars,opt_pars):\n",
    "        super(LocalEnergyOpt, self).__init__()\n",
    "        self.opt_pars = Parameter(torch.tensor(opt_pars, dtype=torch.float, device=device, requires_grad=True))\n",
    "        self.bond_type = Parameter(torch.tensor(fixed_pars['bond_type'], dtype=torch.float, device=device, requires_grad=True))\n",
    "        self.angle_type = Parameter(torch.tensor(fixed_pars['angle_type'], dtype=torch.float, device=device, requires_grad=True))\n",
    "        self.tor_type = Parameter(torch.tensor(fixed_pars['torsion_type'], dtype=torch.float, device=device, requires_grad=True))\n",
    "\n",
    "    def forward(self,X):\n",
    "\n",
    "        X_lengths = X['lengths']\n",
    "        X_features = X['features']\n",
    "\n",
    "        if len(X_lengths.shape) == 1:\n",
    "            X_lengths = X_lengths.unsqueeze(0)\n",
    "            X_features = X_features.unsqueeze(0)\n",
    "\n",
    "        energy = torch.zeros(X_lengths.shape[0],3).to(device)\n",
    "\n",
    "        for i in range(X_lengths.shape[0]):\n",
    "            lengths = X_lengths[i]\n",
    "            features = X_features[i]\n",
    "            if torch.is_tensor(lengths):\n",
    "                lengths = lengths.tolist()\n",
    "            atoms = features[:lengths[0],0].long()\n",
    "            # res_labels\n",
    "            # res_pointer\n",
    "            # mass\n",
    "            # charge\n",
    "            coords = features[:lengths[5],5].reshape(-1,3)\n",
    "            bonds = features[:lengths[6],6].long().reshape(-1,3)\n",
    "            angles = features[:lengths[7],7].long().reshape(-1,4)\n",
    "            tors = features[:lengths[8],8].long().reshape(-1,5)  # all indexes: not necessary to convert to tensors\n",
    "            energy[i,0] = le.bonds_energy(coords,bonds,self.bond_type,self.opt_pars)\n",
    "            energy[i,1] = le.angles_energy(atoms,coords,angles,self.angle_type,self.opt_pars)\n",
    "            energy[i,2] = le.torsions_energy(atoms,coords,tors,self.tor_type,self.opt_pars)\n",
    "\n",
    "        return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset allocated on cpu\n",
      "<bound method Module.parameters of LocalEnergyOpt()>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seq_data = RNASeqDataset(device=device)\n",
    "# train_set, test_set = random_split(seq_data, [], generator=torch.Generator().manual_seed(42))\n",
    "print(f'dataset allocated on {device}')\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "seq_dataloader = DataLoader(seq_data,batch_size=batch_size,shuffle=True,num_workers=1,pin_memory=True)  # all fields must have same length\n",
    "# num_batches = len(seq_dataloader)\n",
    "# print(num_batches)\n",
    "\n",
    "fixed_pars = pickle.load(open('data/SeqCSV/fixed_pars.p', 'rb'))\n",
    "opt_pars = pickle.load(open('data/SeqCSV/pars.p', 'rb'))\n",
    "model = LocalEnergyOpt(fixed_pars,opt_pars).to(device)\n",
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/100 \n",
      "-------------------------\n",
      "Avg loss = 41357.2111, valid batches = 192\n",
      "time for epoch: 2.4895708560943604 \n",
      "\n",
      "epoch 2/100 \n",
      "-------------------------\n",
      "Avg loss = 7544.7395, valid batches = 192\n",
      "time for epoch: 2.310148000717163 \n",
      "\n",
      "epoch 3/100 \n",
      "-------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12967/122455662.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'epoch {index_epoch+1}/{epochs} \\n-------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'time for epoch: {tf-t0} \\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12967/497662717.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Compute prediction error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12967/519681259.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mangles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mtors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# all indexes: not necessary to convert to tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0menergy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbonds_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbonds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbond_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_pars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0menergy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mangles_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mangles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mangle_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_pars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0menergy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorsions_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtor_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_pars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Internship/RNAEnergy/LocalEnergyVct.py\u001b[0m in \u001b[0;36mbonds_energy\u001b[0;34m(coords, bonds, bond_type, pars)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbonds_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbonds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbond_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbonds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbonds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbonds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 1e-7\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor = 0.5, patience = 500, cooldown = 1000, threshold = 1e-12, verbose = True)\n",
    "loss_fn = loss_fn\n",
    "\n",
    "epochs = 100\n",
    "loss_epoch = []\n",
    "for index_epoch in range(epochs):\n",
    "    print(f'epoch {index_epoch+1}/{epochs} \\n-------------------------')\n",
    "    t0 = time.time()\n",
    "    train_loss = train(seq_dataloader, model, loss_fn, optimizer)\n",
    "    tf = time.time()\n",
    "    print(f'time for epoch: {tf-t0} \\n')\n",
    "    loss_epoch.append(train_loss)\n",
    "\n",
    "print(model.opt_pars)\n",
    "torch.save(model.state_dict(), 'data/Results/model_pars.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6b67604d30>]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(loss_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/100 \n",
      "-------------------------\n",
      "Avg loss = 158.2444, valid batches = 192\n",
      "time for epoch: 2.329087495803833 \n",
      "\n",
      "epoch 2/100 \n",
      "-------------------------\n",
      "Avg loss = 160.7974, valid batches = 192\n",
      "time for epoch: 2.288954019546509 \n",
      "\n",
      "epoch 3/100 \n",
      "-------------------------\n",
      "Avg loss = 158.6956, valid batches = 192\n",
      "time for epoch: 2.207125663757324 \n",
      "\n",
      "epoch 4/100 \n",
      "-------------------------\n",
      "Avg loss = 160.6172, valid batches = 192\n",
      "time for epoch: 2.3048746585845947 \n",
      "\n",
      "epoch 5/100 \n",
      "-------------------------\n",
      "Avg loss = 159.4547, valid batches = 192\n",
      "time for epoch: 2.208876132965088 \n",
      "\n",
      "epoch 6/100 \n",
      "-------------------------\n",
      "Avg loss = 156.6195, valid batches = 192\n",
      "time for epoch: 2.2919700145721436 \n",
      "\n",
      "epoch 7/100 \n",
      "-------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12967/2179658600.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'epoch {index_epoch+1}/{epochs} \\n-------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'time for epoch: {tf-t0} \\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12967/497662717.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Compute prediction error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12967/519681259.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0menergy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbonds_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbonds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbond_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_pars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0menergy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mangles_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mangles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mangle_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_pars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0menergy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorsions_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtor_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_pars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0menergy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Internship/RNAEnergy/LocalEnergyVct.py\u001b[0m in \u001b[0;36mtorsions_energy\u001b[0;34m(atoms, coords, tors, tors_type, pars)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mnj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnj\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mnk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrkl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrkj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mnk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnk\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrkj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"data/Results/model_pars.pth\"))\n",
    "\n",
    "epochs = 100\n",
    "loss_epoch = []\n",
    "for index_epoch in range(epochs):\n",
    "    print(f'epoch {index_epoch+1}/{epochs} \\n-------------------------')\n",
    "    t0 = time.time()\n",
    "    train_loss = train(seq_dataloader, model, loss_fn, optimizer)\n",
    "    tf = time.time()\n",
    "    print(f'time for epoch: {tf-t0} \\n')\n",
    "    loss_epoch.append(train_loss)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    # num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    num_batches = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            pred = model(X)\n",
    "            target = get_target(X)\n",
    "            loss = loss_fn(pred, target)\n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "            num_batches += 1\n",
    "            test_loss += loss\n",
    "    test_loss /= num_batches\n",
    "    print(f'Avg test_loss = {test_loss:>0.4f}, valid batches = {num_batches}')\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1327\n",
      "331\n"
     ]
    }
   ],
   "source": [
    "tot_length = len(seq_data)\n",
    "set_length = int(0.2*tot_length)\n",
    "train_set, test_set = random_split(seq_data, [tot_length - set_length, set_length], generator=torch.Generator().manual_seed(42))\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train_set,batch_size=batch_size,shuffle=True,num_workers=1,pin_memory=True)\n",
    "test_dataloader = DataLoader(test_set,batch_size=batch_size,shuffle=True,num_workers=1,pin_memory=True)\n",
    "\n",
    "fixed_pars = pickle.load(open('data/SeqCSV/fixed_pars.p', 'rb'))\n",
    "opt_pars = pickle.load(open('data/SeqCSV/pars.p', 'rb'))\n",
    "model = LocalEnergyOpt(fixed_pars,opt_pars).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/50 \n",
      "-------------------------\n",
      "Avg train_loss = 8694.4095, valid batches = 1313\n",
      "Avg test_loss = 1033.4456, valid batches = 329\n",
      "time for epoch: 3.5768556594848633 \n",
      "\n",
      "epoch 2/50 \n",
      "-------------------------\n",
      "Avg train_loss = 916.8067, valid batches = 1313\n",
      "Avg test_loss = 662.3955, valid batches = 329\n",
      "time for epoch: 3.4069859981536865 \n",
      "\n",
      "epoch 3/50 \n",
      "-------------------------\n",
      "Avg train_loss = 716.3043, valid batches = 1313\n",
      "Avg test_loss = 569.9691, valid batches = 329\n",
      "time for epoch: 3.283191442489624 \n",
      "\n",
      "epoch 4/50 \n",
      "-------------------------\n",
      "Avg train_loss = 634.8976, valid batches = 1313\n",
      "Avg test_loss = 546.5187, valid batches = 329\n",
      "time for epoch: 3.3865725994110107 \n",
      "\n",
      "epoch 5/50 \n",
      "-------------------------\n",
      "Avg train_loss = 579.0503, valid batches = 1313\n",
      "Avg test_loss = 472.3338, valid batches = 329\n",
      "time for epoch: 3.3245086669921875 \n",
      "\n",
      "epoch 6/50 \n",
      "-------------------------\n",
      "Avg train_loss = 536.6402, valid batches = 1313\n",
      "Avg test_loss = 437.0504, valid batches = 329\n",
      "time for epoch: 3.2927021980285645 \n",
      "\n",
      "epoch 7/50 \n",
      "-------------------------\n",
      "Avg train_loss = 498.5295, valid batches = 1313\n",
      "Avg test_loss = 413.3545, valid batches = 329\n",
      "time for epoch: 3.291398286819458 \n",
      "\n",
      "epoch 8/50 \n",
      "-------------------------\n",
      "Avg train_loss = 464.9015, valid batches = 1313\n",
      "Avg test_loss = 385.3960, valid batches = 329\n",
      "time for epoch: 3.289762496948242 \n",
      "\n",
      "epoch 9/50 \n",
      "-------------------------\n",
      "Avg train_loss = 436.3516, valid batches = 1313\n",
      "Avg test_loss = 360.4245, valid batches = 329\n",
      "time for epoch: 3.4112324714660645 \n",
      "\n",
      "epoch 10/50 \n",
      "-------------------------\n",
      "Avg train_loss = 407.0769, valid batches = 1313\n",
      "Avg test_loss = 339.3755, valid batches = 329\n",
      "time for epoch: 3.2978925704956055 \n",
      "\n",
      "epoch 11/50 \n",
      "-------------------------\n",
      "Avg train_loss = 384.0572, valid batches = 1313\n",
      "Avg test_loss = 351.4513, valid batches = 329\n",
      "time for epoch: 3.457603931427002 \n",
      "\n",
      "epoch 12/50 \n",
      "-------------------------\n",
      "Avg train_loss = 360.6466, valid batches = 1313\n",
      "Avg test_loss = 309.5994, valid batches = 329\n",
      "time for epoch: 3.4471993446350098 \n",
      "\n",
      "epoch 13/50 \n",
      "-------------------------\n",
      "Avg train_loss = 342.0832, valid batches = 1313\n",
      "Avg test_loss = 287.5891, valid batches = 329\n",
      "time for epoch: 3.293790817260742 \n",
      "\n",
      "epoch 14/50 \n",
      "-------------------------\n",
      "Avg train_loss = 323.5730, valid batches = 1313\n",
      "Avg test_loss = 270.9633, valid batches = 329\n",
      "time for epoch: 3.7075490951538086 \n",
      "\n",
      "epoch 15/50 \n",
      "-------------------------\n",
      "Avg train_loss = 306.0316, valid batches = 1313\n",
      "Avg test_loss = 257.7887, valid batches = 329\n",
      "time for epoch: 3.720393180847168 \n",
      "\n",
      "epoch 16/50 \n",
      "-------------------------\n",
      "Avg train_loss = 290.7216, valid batches = 1313\n",
      "Avg test_loss = 246.0018, valid batches = 329\n",
      "time for epoch: 3.6754627227783203 \n",
      "\n",
      "epoch 17/50 \n",
      "-------------------------\n",
      "Avg train_loss = 275.5675, valid batches = 1313\n",
      "Avg test_loss = 235.4335, valid batches = 329\n",
      "time for epoch: 3.7087292671203613 \n",
      "\n",
      "epoch 18/50 \n",
      "-------------------------\n",
      "Avg train_loss = 263.1623, valid batches = 1313\n",
      "Avg test_loss = 229.5199, valid batches = 329\n",
      "time for epoch: 3.619326114654541 \n",
      "\n",
      "epoch 19/50 \n",
      "-------------------------\n",
      "Avg train_loss = 250.6034, valid batches = 1313\n",
      "Avg test_loss = 213.4708, valid batches = 329\n",
      "time for epoch: 3.520092725753784 \n",
      "\n",
      "epoch 20/50 \n",
      "-------------------------\n",
      "Avg train_loss = 239.0981, valid batches = 1313\n",
      "Avg test_loss = 204.5787, valid batches = 329\n",
      "time for epoch: 3.5891835689544678 \n",
      "\n",
      "epoch 21/50 \n",
      "-------------------------\n",
      "Avg train_loss = 229.0555, valid batches = 1313\n",
      "Avg test_loss = 201.2761, valid batches = 329\n",
      "time for epoch: 3.5136120319366455 \n",
      "\n",
      "epoch 22/50 \n",
      "-------------------------\n",
      "Avg train_loss = 219.5963, valid batches = 1313\n",
      "Avg test_loss = 190.4751, valid batches = 329\n",
      "time for epoch: 3.562304973602295 \n",
      "\n",
      "epoch 23/50 \n",
      "-------------------------\n",
      "Avg train_loss = 210.2906, valid batches = 1313\n",
      "Avg test_loss = 185.4733, valid batches = 329\n",
      "time for epoch: 3.5581347942352295 \n",
      "\n",
      "epoch 24/50 \n",
      "-------------------------\n",
      "Avg train_loss = 202.2134, valid batches = 1313\n",
      "Avg test_loss = 185.4107, valid batches = 329\n",
      "time for epoch: 3.500720977783203 \n",
      "\n",
      "epoch 25/50 \n",
      "-------------------------\n",
      "Avg train_loss = 194.5139, valid batches = 1313\n",
      "Avg test_loss = 170.2866, valid batches = 329\n",
      "time for epoch: 3.6031692028045654 \n",
      "\n",
      "epoch 26/50 \n",
      "-------------------------\n",
      "Avg train_loss = 187.4588, valid batches = 1313\n",
      "Avg test_loss = 179.7732, valid batches = 329\n",
      "time for epoch: 3.5415656566619873 \n",
      "\n",
      "epoch 27/50 \n",
      "-------------------------\n",
      "Avg train_loss = 181.0379, valid batches = 1313\n",
      "Avg test_loss = 171.4298, valid batches = 329\n",
      "time for epoch: 3.597433567047119 \n",
      "\n",
      "epoch 28/50 \n",
      "-------------------------\n",
      "Avg train_loss = 175.0580, valid batches = 1313\n",
      "Avg test_loss = 153.1273, valid batches = 329\n",
      "time for epoch: 3.5447511672973633 \n",
      "\n",
      "epoch 29/50 \n",
      "-------------------------\n",
      "Avg train_loss = 169.5941, valid batches = 1313\n",
      "Avg test_loss = 163.4425, valid batches = 329\n",
      "time for epoch: 3.71415114402771 \n",
      "\n",
      "epoch 30/50 \n",
      "-------------------------\n",
      "Avg train_loss = 164.0831, valid batches = 1313\n",
      "Avg test_loss = 155.5242, valid batches = 329\n",
      "time for epoch: 3.5330395698547363 \n",
      "\n",
      "epoch 31/50 \n",
      "-------------------------\n",
      "Avg train_loss = 159.3687, valid batches = 1313\n",
      "Avg test_loss = 140.4314, valid batches = 329\n",
      "time for epoch: 3.615936517715454 \n",
      "\n",
      "epoch 32/50 \n",
      "-------------------------\n",
      "Avg train_loss = 155.1439, valid batches = 1313\n",
      "Avg test_loss = 142.8057, valid batches = 329\n",
      "time for epoch: 3.594515085220337 \n",
      "\n",
      "epoch 33/50 \n",
      "-------------------------\n",
      "Avg train_loss = 151.1882, valid batches = 1313\n",
      "Avg test_loss = 133.8071, valid batches = 329\n",
      "time for epoch: 3.516953945159912 \n",
      "\n",
      "epoch 34/50 \n",
      "-------------------------\n",
      "Avg train_loss = 147.0069, valid batches = 1313\n",
      "Avg test_loss = 130.3284, valid batches = 329\n",
      "time for epoch: 3.82283353805542 \n",
      "\n",
      "epoch 35/50 \n",
      "-------------------------\n",
      "Avg train_loss = 142.3775, valid batches = 1313\n",
      "Avg test_loss = 134.5690, valid batches = 329\n",
      "time for epoch: 3.7555124759674072 \n",
      "\n",
      "epoch 36/50 \n",
      "-------------------------\n",
      "Avg train_loss = 138.9259, valid batches = 1313\n",
      "Avg test_loss = 135.9576, valid batches = 329\n",
      "time for epoch: 3.558364152908325 \n",
      "\n",
      "epoch 37/50 \n",
      "-------------------------\n",
      "Avg train_loss = 135.6878, valid batches = 1313\n",
      "Avg test_loss = 121.9415, valid batches = 329\n",
      "time for epoch: 3.535860776901245 \n",
      "\n",
      "epoch 38/50 \n",
      "-------------------------\n",
      "Avg train_loss = 133.4321, valid batches = 1313\n",
      "Avg test_loss = 119.6608, valid batches = 329\n",
      "time for epoch: 3.622304677963257 \n",
      "\n",
      "epoch 39/50 \n",
      "-------------------------\n",
      "Avg train_loss = 130.3974, valid batches = 1313\n",
      "Avg test_loss = 120.9502, valid batches = 329\n",
      "time for epoch: 3.679180860519409 \n",
      "\n",
      "epoch 40/50 \n",
      "-------------------------\n",
      "Avg train_loss = 127.5184, valid batches = 1313\n",
      "Avg test_loss = 122.1267, valid batches = 329\n",
      "time for epoch: 3.7068803310394287 \n",
      "\n",
      "epoch 41/50 \n",
      "-------------------------\n",
      "Avg train_loss = 125.3019, valid batches = 1313\n",
      "Avg test_loss = 122.9076, valid batches = 329\n",
      "time for epoch: 3.522623300552368 \n",
      "\n",
      "epoch 42/50 \n",
      "-------------------------\n",
      "Avg train_loss = 121.9192, valid batches = 1313\n",
      "Avg test_loss = 111.8564, valid batches = 329\n",
      "time for epoch: 3.5740907192230225 \n",
      "\n",
      "epoch 43/50 \n",
      "-------------------------\n",
      "Avg train_loss = 120.2303, valid batches = 1313\n",
      "Avg test_loss = 111.5874, valid batches = 329\n",
      "time for epoch: 3.5449702739715576 \n",
      "\n",
      "epoch 44/50 \n",
      "-------------------------\n",
      "Avg train_loss = 118.2226, valid batches = 1313\n",
      "Avg test_loss = 114.0821, valid batches = 329\n",
      "time for epoch: 3.871173620223999 \n",
      "\n",
      "epoch 45/50 \n",
      "-------------------------\n",
      "Avg train_loss = 116.3002, valid batches = 1313\n",
      "Avg test_loss = 105.2797, valid batches = 329\n",
      "time for epoch: 3.710258960723877 \n",
      "\n",
      "epoch 46/50 \n",
      "-------------------------\n",
      "Avg train_loss = 113.7312, valid batches = 1313\n",
      "Avg test_loss = 108.6582, valid batches = 329\n",
      "time for epoch: 3.631395101547241 \n",
      "\n",
      "epoch 47/50 \n",
      "-------------------------\n",
      "Avg train_loss = 112.0510, valid batches = 1313\n",
      "Avg test_loss = 102.2075, valid batches = 329\n",
      "time for epoch: 3.6354944705963135 \n",
      "\n",
      "epoch 48/50 \n",
      "-------------------------\n",
      "Avg train_loss = 111.0006, valid batches = 1313\n",
      "Avg test_loss = 102.7000, valid batches = 329\n",
      "time for epoch: 3.7427427768707275 \n",
      "\n",
      "epoch 49/50 \n",
      "-------------------------\n",
      "Avg train_loss = 108.9959, valid batches = 1313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test_loss = 99.0555, valid batches = 329\n",
      "time for epoch: 3.84403920173645 \n",
      "\n",
      "epoch 50/50 \n",
      "-------------------------\n",
      "Avg train_loss = 107.1122, valid batches = 1313\n",
      "Avg test_loss = 97.9286, valid batches = 329\n",
      "time for epoch: 3.7500054836273193 \n",
      "\n",
      "Parameter containing:\n",
      "tensor([9.0741e-01, 2.0490e+00, 1.5187e+00, 2.3543e+00, 4.1887e+00, 4.5984e+00,\n",
      "        4.7527e+00, 5.5910e+00, 2.0993e+00, 1.6734e+00, 1.5223e+01, 1.0000e+00,\n",
      "        2.8000e+00, 2.5050e+00, 1.8260e+00, 3.9320e+00, 4.3090e+00, 4.7750e+00,\n",
      "        4.5460e+00, 2.8210e+00, 3.8130e+00, 3.0100e+00, 9.0800e-01, 3.0000e+00,\n",
      "        4.0000e+00, 2.2570e+00, 4.8000e-01, 5.0000e-01, 4.2209e+00, 1.0815e+01,\n",
      "        1.1133e+01, 5.8970e+00, 4.8332e-01, 6.9996e-01, 3.7835e-01, 3.7293e-01,\n",
      "        3.3325e-01, 2.7306e-01, 1.2000e+00, 1.5000e+00, 4.0000e-01, 1.8000e+00,\n",
      "        8.0000e-01, 1.4231e+02, 1.0000e+00, 0.0000e+00, 3.3696e-02],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-7\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor = 0.5, patience = 500, cooldown = 1000, threshold = 1e-12, verbose = True)\n",
    "loss_fn = loss_fn\n",
    "\n",
    "epochs = 50\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "for index_epoch in range(epochs):\n",
    "    print(f'epoch {index_epoch+1}/{epochs} \\n-------------------------')\n",
    "    t0 = time.time()\n",
    "    train_tmp = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_tmp = test(test_dataloader, model, loss_fn)\n",
    "    tf = time.time()\n",
    "    print(f'time for epoch: {tf-t0} \\n')\n",
    "    train_loss.append(train_tmp)\n",
    "    test_loss.append(test_tmp)\n",
    "    \n",
    "print(model.opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogy(train_loss,label='train')\n",
    "plt.semilogy(test_loss,label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 13.2898,  36.8922, 169.6867])\n",
      "tensor([[  8.8202,  37.0709, 164.1852]])\n",
      "\n",
      "\n",
      "tensor([ 12.6754,  34.2673, 172.8444])\n",
      "tensor([[  6.7461,  34.7743, 180.1666]])\n",
      "\n",
      "\n",
      "tensor([ 10.9180,  40.8229, 194.3014])\n",
      "tensor([[ 30.2294,  43.3862, 187.4200]])\n",
      "\n",
      "\n",
      "tensor([ 12.5252,  37.0046, 175.1599])\n",
      "tensor([[  9.6312,  37.9470, 187.2590]])\n",
      "\n",
      "\n",
      "tensor([ 11.2705,  43.4584, 203.6229])\n",
      "tensor([[ 16.3466,  41.8296, 191.5134]])\n",
      "\n",
      "\n",
      "tensor([ 11.2424,  37.5109, 167.7077])\n",
      "tensor([[ 16.3738,  35.8468, 167.8327]])\n",
      "\n",
      "\n",
      "tensor([ 11.9711,  32.8871, 181.5240])\n",
      "tensor([[  6.9003,  30.9838, 184.0946]])\n",
      "\n",
      "\n",
      "tensor([ 11.9789,  46.5330, 184.3864])\n",
      "tensor([[ 15.6258,  27.5016, 183.5974]])\n",
      "\n",
      "\n",
      "tensor([ 10.1928,  37.2153, 160.4371])\n",
      "tensor([[ 15.7766,  37.9461, 165.8736]])\n",
      "\n",
      "\n",
      "tensor([ 13.9800,  35.8230, 165.1962])\n",
      "tensor([[ 10.5354,  35.9227, 176.7791]])\n",
      "\n",
      "\n",
      "Parameter containing:\n",
      "tensor([9.0741e-01, 2.0490e+00, 1.5187e+00, 2.3543e+00, 4.1887e+00, 4.5984e+00,\n",
      "        4.7527e+00, 5.5910e+00, 2.0993e+00, 1.6734e+00, 1.5223e+01, 1.0000e+00,\n",
      "        2.8000e+00, 2.5050e+00, 1.8260e+00, 3.9320e+00, 4.3090e+00, 4.7750e+00,\n",
      "        4.5460e+00, 2.8210e+00, 3.8130e+00, 3.0100e+00, 9.0800e-01, 3.0000e+00,\n",
      "        4.0000e+00, 2.2570e+00, 4.8000e-01, 5.0000e-01, 4.2209e+00, 1.0815e+01,\n",
      "        1.1133e+01, 5.8970e+00, 4.8332e-01, 6.9996e-01, 3.7835e-01, 3.7293e-01,\n",
      "        3.3325e-01, 2.7306e-01, 1.2000e+00, 1.5000e+00, 4.0000e-01, 1.8000e+00,\n",
      "        8.0000e-01, 1.4231e+02, 1.0000e+00, 0.0000e+00, 3.3696e-02],\n",
      "       requires_grad=True)\n",
      "{'bond_type': Parameter containing:\n",
      "tensor([[ 29.9178,   3.6555],\n",
      "        [200.0000,   2.3347],\n",
      "        [200.0000,   2.6670],\n",
      "        [200.0000,   2.6542],\n",
      "        [200.0000,   3.0756],\n",
      "        [200.0000,   3.0352],\n",
      "        [200.0000,   2.4840],\n",
      "        [200.0000,   2.1981],\n",
      "        [200.0000,   1.5298],\n",
      "        [200.0000,   1.6115],\n",
      "        [200.0000,   1.4233],\n",
      "        [ 40.0000,  12.0000],\n",
      "        [ 10.0000,  12.0000],\n",
      "        [ 10.0000,  12.0000],\n",
      "        [ 10.0000,  12.0000]], requires_grad=True), 'angle_type': Parameter containing:\n",
      "tensor([[ 69.9998,   2.2171],\n",
      "        [ 69.9998,   2.3643],\n",
      "        [ 69.9996,   2.2111],\n",
      "        [ 70.0000,   2.3368],\n",
      "        [120.0000,   2.0379],\n",
      "        [120.0000,   1.9441],\n",
      "        [ 70.0000,   2.1299],\n",
      "        [ 70.0000,   1.9762],\n",
      "        [ 69.9938,   1.9497],\n",
      "        [ 49.9931,   1.9957],\n",
      "        [ 69.9970,   1.8542],\n",
      "        [ 99.9990,   2.1123],\n",
      "        [ 80.0000,   3.1416]], requires_grad=True), 'torsion_type': Parameter containing:\n",
      "tensor([[ 8.3727e-01,  8.9422e-01, -3.6457e-01],\n",
      "        [ 3.5826e-01,  1.0986e+00, -2.8160e+00],\n",
      "        [ 9.4707e-01,  9.5266e-01, -3.5060e-01],\n",
      "        [ 3.6489e-01,  1.0819e+00, -2.8140e+00],\n",
      "        [ 9.9666e-01,  1.1174e+00, -2.8591e+00],\n",
      "        [ 9.9466e-01,  1.1051e+00, -2.8783e+00],\n",
      "        [ 1.0068e+00,  1.2007e+00, -2.5287e+00],\n",
      "        [ 2.4056e-01,  1.0051e+00,  2.5812e+00],\n",
      "        [ 1.0017e+00,  1.1756e+00, -2.5299e+00],\n",
      "        [ 1.0048e+00,  1.0807e+00, -2.4365e+00],\n",
      "        [ 4.1279e-01,  1.0750e+00,  2.4519e+00],\n",
      "        [ 9.9753e-01,  1.0988e+00, -2.4879e+00],\n",
      "        [ 3.8209e-01,  1.1309e+00,  2.4992e+00],\n",
      "        [ 1.0837e+00,  5.2677e-01,  1.8527e+00],\n",
      "        [ 9.7172e-01,  4.9235e-01,  1.8637e+00],\n",
      "        [ 1.2024e+00,  5.0054e-01,  1.8612e+00],\n",
      "        [ 1.1619e+00,  4.7711e-01,  1.8662e+00],\n",
      "        [ 1.1927e+00,  1.1384e+00,  3.0847e-01],\n",
      "        [ 1.1820e+00,  1.0644e+00,  2.7509e+00],\n",
      "        [ 1.0167e+00,  3.0122e+00,  1.5661e+00],\n",
      "        [ 1.0359e+00,  1.0282e+00,  2.6004e+00],\n",
      "        [ 3.4050e-01,  1.0109e+00, -1.6210e-03],\n",
      "        [ 1.5002e-01,  2.0052e+00,  3.1405e+00],\n",
      "        [ 8.5781e-01,  3.0098e+00, -3.0939e-03],\n",
      "        [ 1.0158e+00,  1.9883e+00,  1.3246e-03]], requires_grad=True)}\n"
     ]
    }
   ],
   "source": [
    "pars = model.opt_pars\n",
    "fixed_pars = {'bond_type': model.bond_type,\n",
    "              'angle_type': model.angle_type,\n",
    "              'torsion_type': model.tor_type\n",
    "             }\n",
    "\n",
    "model2 = LocalEnergyOpt(fixed_pars,pars).to(device)\n",
    "\n",
    "for i in range(10):\n",
    "    a = test_set[i]\n",
    "    amber_en = a['features'][0:3,9]\n",
    "    print(amber_en)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hire_en = model2(a)\n",
    "    print(hire_en)\n",
    "    print('\\n')\n",
    "    \n",
    "print(pars)\n",
    "print(fixed_pars)\n",
    "torch.save(model.state_dict(), 'data/Results/model_pars.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/100 \n",
      "-------------------------\n",
      "Avg train_loss = 106.2746, valid batches = 1313\n",
      "Avg test_loss = 98.9340, valid batches = 329\n",
      "time for epoch: 3.312833070755005 \n",
      "\n",
      "epoch 2/100 \n",
      "-------------------------\n",
      "Avg train_loss = 105.2661, valid batches = 1313\n",
      "Avg test_loss = 123.3520, valid batches = 329\n",
      "time for epoch: 3.260486364364624 \n",
      "\n",
      "epoch 3/100 \n",
      "-------------------------\n",
      "Avg train_loss = 102.9002, valid batches = 1313\n",
      "Avg test_loss = 94.3369, valid batches = 329\n",
      "time for epoch: 3.4366748332977295 \n",
      "\n",
      "epoch 4/100 \n",
      "-------------------------\n",
      "Avg train_loss = 101.1255, valid batches = 1313\n",
      "Avg test_loss = 92.5675, valid batches = 329\n",
      "time for epoch: 3.3224596977233887 \n",
      "\n",
      "epoch 5/100 \n",
      "-------------------------\n",
      "Avg train_loss = 100.2480, valid batches = 1313\n",
      "Avg test_loss = 94.0634, valid batches = 329\n",
      "time for epoch: 3.2731313705444336 \n",
      "\n",
      "epoch 6/100 \n",
      "-------------------------\n",
      "Avg train_loss = 98.8574, valid batches = 1313\n",
      "Avg test_loss = 96.1350, valid batches = 329\n",
      "time for epoch: 3.3278958797454834 \n",
      "\n",
      "epoch 7/100 \n",
      "-------------------------\n",
      "Avg train_loss = 98.1649, valid batches = 1313\n",
      "Avg test_loss = 88.7100, valid batches = 329\n",
      "time for epoch: 3.2748639583587646 \n",
      "\n",
      "epoch 8/100 \n",
      "-------------------------\n",
      "Avg train_loss = 96.2508, valid batches = 1313\n",
      "Avg test_loss = 88.2915, valid batches = 329\n",
      "time for epoch: 3.378021717071533 \n",
      "\n",
      "epoch 9/100 \n",
      "-------------------------\n",
      "Avg train_loss = 95.8971, valid batches = 1313\n",
      "Avg test_loss = 87.7050, valid batches = 329\n",
      "time for epoch: 3.3090920448303223 \n",
      "\n",
      "epoch 10/100 \n",
      "-------------------------\n",
      "Avg train_loss = 94.7096, valid batches = 1313\n",
      "Avg test_loss = 88.7409, valid batches = 329\n",
      "time for epoch: 3.2776641845703125 \n",
      "\n",
      "epoch 11/100 \n",
      "-------------------------\n",
      "Avg train_loss = 93.9277, valid batches = 1313\n",
      "Avg test_loss = 88.3689, valid batches = 329\n",
      "time for epoch: 3.345640182495117 \n",
      "\n",
      "epoch 12/100 \n",
      "-------------------------\n",
      "Avg train_loss = 92.8499, valid batches = 1313\n",
      "Avg test_loss = 106.3506, valid batches = 329\n",
      "time for epoch: 3.30595064163208 \n",
      "\n",
      "epoch 13/100 \n",
      "-------------------------\n",
      "Avg train_loss = 91.8887, valid batches = 1313\n",
      "Avg test_loss = 84.5279, valid batches = 329\n",
      "time for epoch: 3.2991583347320557 \n",
      "\n",
      "epoch 14/100 \n",
      "-------------------------\n",
      "Avg train_loss = 90.6482, valid batches = 1313\n",
      "Avg test_loss = 85.3753, valid batches = 329\n",
      "time for epoch: 3.379760980606079 \n",
      "\n",
      "epoch 15/100 \n",
      "-------------------------\n",
      "Avg train_loss = 89.8844, valid batches = 1313\n",
      "Avg test_loss = 83.6824, valid batches = 329\n",
      "time for epoch: 3.354722738265991 \n",
      "\n",
      "epoch 16/100 \n",
      "-------------------------\n",
      "Avg train_loss = 87.5423, valid batches = 1313\n",
      "Avg test_loss = 80.8157, valid batches = 329\n",
      "time for epoch: 3.520667552947998 \n",
      "\n",
      "epoch 17/100 \n",
      "-------------------------\n",
      "Avg train_loss = 88.5477, valid batches = 1313\n",
      "Avg test_loss = 82.4800, valid batches = 329\n",
      "time for epoch: 3.47609806060791 \n",
      "\n",
      "epoch 18/100 \n",
      "-------------------------\n",
      "Avg train_loss = 87.4038, valid batches = 1313\n",
      "Avg test_loss = 88.6200, valid batches = 329\n",
      "time for epoch: 3.528561592102051 \n",
      "\n",
      "epoch 19/100 \n",
      "-------------------------\n",
      "Avg train_loss = 87.3796, valid batches = 1313\n",
      "Avg test_loss = 84.0011, valid batches = 329\n",
      "time for epoch: 3.444793224334717 \n",
      "\n",
      "epoch 20/100 \n",
      "-------------------------\n",
      "Avg train_loss = 86.4136, valid batches = 1313\n",
      "Avg test_loss = 81.7140, valid batches = 329\n",
      "time for epoch: 3.5504517555236816 \n",
      "\n",
      "epoch 21/100 \n",
      "-------------------------\n",
      "Avg train_loss = 85.8073, valid batches = 1313\n",
      "Avg test_loss = 77.3035, valid batches = 329\n",
      "time for epoch: 3.4770963191986084 \n",
      "\n",
      "epoch 22/100 \n",
      "-------------------------\n",
      "Avg train_loss = 85.0941, valid batches = 1313\n",
      "Avg test_loss = 76.3993, valid batches = 329\n",
      "time for epoch: 3.481271266937256 \n",
      "\n",
      "epoch 23/100 \n",
      "-------------------------\n",
      "Avg train_loss = 83.9212, valid batches = 1313\n",
      "Avg test_loss = 80.3806, valid batches = 329\n",
      "time for epoch: 3.518930196762085 \n",
      "\n",
      "epoch 24/100 \n",
      "-------------------------\n",
      "Avg train_loss = 82.7191, valid batches = 1313\n",
      "Avg test_loss = 75.7106, valid batches = 329\n",
      "time for epoch: 3.5012638568878174 \n",
      "\n",
      "epoch 25/100 \n",
      "-------------------------\n",
      "Avg train_loss = 82.8953, valid batches = 1313\n",
      "Avg test_loss = 74.6444, valid batches = 329\n",
      "time for epoch: 3.462831735610962 \n",
      "\n",
      "epoch 26/100 \n",
      "-------------------------\n",
      "Avg train_loss = 82.2327, valid batches = 1313\n",
      "Avg test_loss = 73.8557, valid batches = 329\n",
      "time for epoch: 3.509216547012329 \n",
      "\n",
      "epoch 27/100 \n",
      "-------------------------\n",
      "Avg train_loss = 81.4354, valid batches = 1313\n",
      "Avg test_loss = 73.5091, valid batches = 329\n",
      "time for epoch: 3.4571077823638916 \n",
      "\n",
      "epoch 28/100 \n",
      "-------------------------\n",
      "Avg train_loss = 81.3438, valid batches = 1313\n",
      "Avg test_loss = 73.8535, valid batches = 329\n",
      "time for epoch: 3.4888646602630615 \n",
      "\n",
      "epoch 29/100 \n",
      "-------------------------\n",
      "Avg train_loss = 80.6613, valid batches = 1313\n",
      "Avg test_loss = 74.1024, valid batches = 329\n",
      "time for epoch: 3.5403478145599365 \n",
      "\n",
      "epoch 30/100 \n",
      "-------------------------\n",
      "Avg train_loss = 79.5305, valid batches = 1313\n",
      "Avg test_loss = 75.4267, valid batches = 329\n",
      "time for epoch: 3.4904370307922363 \n",
      "\n",
      "epoch 31/100 \n",
      "-------------------------\n",
      "Avg train_loss = 79.3815, valid batches = 1313\n",
      "Avg test_loss = 72.5425, valid batches = 329\n",
      "time for epoch: 3.4259488582611084 \n",
      "\n",
      "epoch 32/100 \n",
      "-------------------------\n",
      "Avg train_loss = 78.7287, valid batches = 1313\n",
      "Avg test_loss = 70.7426, valid batches = 329\n",
      "time for epoch: 3.4888596534729004 \n",
      "\n",
      "epoch 33/100 \n",
      "-------------------------\n",
      "Avg train_loss = 77.9963, valid batches = 1313\n",
      "Avg test_loss = 72.2967, valid batches = 329\n",
      "time for epoch: 3.451125383377075 \n",
      "\n",
      "epoch 34/100 \n",
      "-------------------------\n",
      "Avg train_loss = 77.9559, valid batches = 1313\n",
      "Avg test_loss = 69.7638, valid batches = 329\n",
      "time for epoch: 3.508265733718872 \n",
      "\n",
      "epoch 35/100 \n",
      "-------------------------\n",
      "Avg train_loss = 77.2735, valid batches = 1313\n",
      "Avg test_loss = 70.4885, valid batches = 329\n",
      "time for epoch: 3.4859557151794434 \n",
      "\n",
      "epoch 36/100 \n",
      "-------------------------\n",
      "Avg train_loss = 76.3020, valid batches = 1313\n",
      "Avg test_loss = 82.8609, valid batches = 329\n",
      "time for epoch: 3.466062307357788 \n",
      "\n",
      "epoch 37/100 \n",
      "-------------------------\n",
      "Avg train_loss = 76.6144, valid batches = 1313\n",
      "Avg test_loss = 68.6013, valid batches = 329\n",
      "time for epoch: 3.591567039489746 \n",
      "\n",
      "epoch 38/100 \n",
      "-------------------------\n",
      "Avg train_loss = 75.5314, valid batches = 1313\n",
      "Avg test_loss = 69.3676, valid batches = 329\n",
      "time for epoch: 3.4635682106018066 \n",
      "\n",
      "epoch 39/100 \n",
      "-------------------------\n",
      "Avg train_loss = 75.0753, valid batches = 1313\n",
      "Avg test_loss = 67.7439, valid batches = 329\n",
      "time for epoch: 3.450871706008911 \n",
      "\n",
      "epoch 40/100 \n",
      "-------------------------\n",
      "Avg train_loss = 75.2697, valid batches = 1313\n",
      "Avg test_loss = 70.7708, valid batches = 329\n",
      "time for epoch: 3.543602466583252 \n",
      "\n",
      "epoch 41/100 \n",
      "-------------------------\n",
      "Avg train_loss = 74.2098, valid batches = 1313\n",
      "Avg test_loss = 67.0645, valid batches = 329\n",
      "time for epoch: 3.4537951946258545 \n",
      "\n",
      "epoch 42/100 \n",
      "-------------------------\n",
      "Avg train_loss = 74.7668, valid batches = 1313\n",
      "Avg test_loss = 67.1807, valid batches = 329\n",
      "time for epoch: 3.5107431411743164 \n",
      "\n",
      "epoch 43/100 \n",
      "-------------------------\n",
      "Avg train_loss = 74.3730, valid batches = 1313\n",
      "Avg test_loss = 65.5917, valid batches = 329\n",
      "time for epoch: 3.478909969329834 \n",
      "\n",
      "epoch 44/100 \n",
      "-------------------------\n",
      "Avg train_loss = 73.3013, valid batches = 1313\n",
      "Avg test_loss = 69.0383, valid batches = 329\n",
      "time for epoch: 3.5076100826263428 \n",
      "\n",
      "epoch 45/100 \n",
      "-------------------------\n",
      "Avg train_loss = 73.6571, valid batches = 1313\n",
      "Avg test_loss = 66.6803, valid batches = 329\n",
      "time for epoch: 3.5867273807525635 \n",
      "\n",
      "epoch 46/100 \n",
      "-------------------------\n",
      "Avg train_loss = 72.6799, valid batches = 1313\n",
      "Avg test_loss = 65.2436, valid batches = 329\n",
      "time for epoch: 3.477445602416992 \n",
      "\n",
      "epoch 47/100 \n",
      "-------------------------\n",
      "Avg train_loss = 72.3714, valid batches = 1313\n",
      "Avg test_loss = 69.6176, valid batches = 329\n",
      "time for epoch: 3.467712163925171 \n",
      "\n",
      "epoch 48/100 \n",
      "-------------------------\n",
      "Avg train_loss = 72.1531, valid batches = 1313\n",
      "Avg test_loss = 81.5030, valid batches = 329\n",
      "time for epoch: 3.553260564804077 \n",
      "\n",
      "epoch 49/100 \n",
      "-------------------------\n",
      "Avg train_loss = 71.2080, valid batches = 1313\n",
      "Avg test_loss = 63.4532, valid batches = 329\n",
      "time for epoch: 3.799128293991089 \n",
      "\n",
      "epoch 50/100 \n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train_loss = 70.9138, valid batches = 1313\n",
      "Avg test_loss = 64.5076, valid batches = 329\n",
      "time for epoch: 3.5459752082824707 \n",
      "\n",
      "epoch 51/100 \n",
      "-------------------------\n",
      "Avg train_loss = 70.7156, valid batches = 1313\n",
      "Avg test_loss = 64.6091, valid batches = 329\n",
      "time for epoch: 3.5068671703338623 \n",
      "\n",
      "epoch 52/100 \n",
      "-------------------------\n",
      "Avg train_loss = 70.0085, valid batches = 1313\n",
      "Avg test_loss = 63.0843, valid batches = 329\n",
      "time for epoch: 3.5855045318603516 \n",
      "\n",
      "epoch 53/100 \n",
      "-------------------------\n",
      "Avg train_loss = 70.1742, valid batches = 1313\n",
      "Avg test_loss = 80.4191, valid batches = 329\n",
      "time for epoch: 3.5875353813171387 \n",
      "\n",
      "epoch 54/100 \n",
      "-------------------------\n",
      "Avg train_loss = 70.2760, valid batches = 1313\n",
      "Avg test_loss = 66.1394, valid batches = 329\n",
      "time for epoch: 3.499817371368408 \n",
      "\n",
      "epoch 55/100 \n",
      "-------------------------\n",
      "Avg train_loss = 69.6677, valid batches = 1313\n",
      "Avg test_loss = 61.5837, valid batches = 329\n",
      "time for epoch: 3.528301239013672 \n",
      "\n",
      "epoch 56/100 \n",
      "-------------------------\n",
      "Avg train_loss = 69.4967, valid batches = 1313\n",
      "Avg test_loss = 61.0684, valid batches = 329\n",
      "time for epoch: 3.633326768875122 \n",
      "\n",
      "epoch 57/100 \n",
      "-------------------------\n",
      "Avg train_loss = 69.7745, valid batches = 1313\n",
      "Avg test_loss = 61.1486, valid batches = 329\n",
      "time for epoch: 3.5000104904174805 \n",
      "\n",
      "epoch 58/100 \n",
      "-------------------------\n",
      "Avg train_loss = 69.3592, valid batches = 1313\n",
      "Avg test_loss = 60.3329, valid batches = 329\n",
      "time for epoch: 3.505401134490967 \n",
      "\n",
      "epoch 59/100 \n",
      "-------------------------\n",
      "Avg train_loss = 68.6257, valid batches = 1313\n",
      "Avg test_loss = 59.9123, valid batches = 329\n",
      "time for epoch: 3.489070177078247 \n",
      "\n",
      "epoch 60/100 \n",
      "-------------------------\n",
      "Avg train_loss = 68.5894, valid batches = 1313\n",
      "Avg test_loss = 59.9021, valid batches = 329\n",
      "time for epoch: 3.5444867610931396 \n",
      "\n",
      "epoch 61/100 \n",
      "-------------------------\n",
      "Avg train_loss = 67.0424, valid batches = 1313\n",
      "Avg test_loss = 61.6988, valid batches = 329\n",
      "time for epoch: 3.498180866241455 \n",
      "\n",
      "epoch 62/100 \n",
      "-------------------------\n",
      "Avg train_loss = 68.3492, valid batches = 1313\n",
      "Avg test_loss = 63.4675, valid batches = 329\n",
      "time for epoch: 3.496081590652466 \n",
      "\n",
      "epoch 63/100 \n",
      "-------------------------\n",
      "Avg train_loss = 67.3753, valid batches = 1313\n",
      "Avg test_loss = 58.5789, valid batches = 329\n",
      "time for epoch: 3.54148268699646 \n",
      "\n",
      "epoch 64/100 \n",
      "-------------------------\n",
      "Avg train_loss = 67.2703, valid batches = 1313\n",
      "Avg test_loss = 58.4316, valid batches = 329\n",
      "time for epoch: 3.480623960494995 \n",
      "\n",
      "epoch 65/100 \n",
      "-------------------------\n",
      "Avg train_loss = 66.6198, valid batches = 1313\n",
      "Avg test_loss = 86.1600, valid batches = 329\n",
      "time for epoch: 3.665416717529297 \n",
      "\n",
      "epoch 66/100 \n",
      "-------------------------\n",
      "Avg train_loss = 66.3057, valid batches = 1313\n",
      "Avg test_loss = 64.9057, valid batches = 329\n",
      "time for epoch: 3.6746015548706055 \n",
      "\n",
      "epoch 67/100 \n",
      "-------------------------\n",
      "Avg train_loss = 65.5009, valid batches = 1313\n",
      "Avg test_loss = 64.7007, valid batches = 329\n",
      "time for epoch: 3.7693772315979004 \n",
      "\n",
      "epoch 68/100 \n",
      "-------------------------\n",
      "Avg train_loss = 65.5761, valid batches = 1313\n",
      "Avg test_loss = 59.6655, valid batches = 329\n",
      "time for epoch: 3.6287131309509277 \n",
      "\n",
      "epoch 69/100 \n",
      "-------------------------\n",
      "Avg train_loss = 65.1816, valid batches = 1313\n",
      "Avg test_loss = 69.4047, valid batches = 329\n",
      "time for epoch: 3.6759040355682373 \n",
      "\n",
      "epoch 70/100 \n",
      "-------------------------\n",
      "Avg train_loss = 64.8954, valid batches = 1313\n",
      "Avg test_loss = 82.4966, valid batches = 329\n",
      "time for epoch: 3.5711941719055176 \n",
      "\n",
      "epoch 71/100 \n",
      "-------------------------\n",
      "Avg train_loss = 64.8460, valid batches = 1313\n",
      "Avg test_loss = 62.4122, valid batches = 329\n",
      "time for epoch: 3.5736827850341797 \n",
      "\n",
      "epoch 72/100 \n",
      "-------------------------\n",
      "Avg train_loss = 64.9014, valid batches = 1313\n",
      "Avg test_loss = 108.0469, valid batches = 329\n",
      "time for epoch: 3.5394504070281982 \n",
      "\n",
      "epoch 73/100 \n",
      "-------------------------\n",
      "Avg train_loss = 63.7987, valid batches = 1313\n",
      "Avg test_loss = 81.6526, valid batches = 329\n",
      "time for epoch: 3.634906053543091 \n",
      "\n",
      "epoch 74/100 \n",
      "-------------------------\n",
      "Avg train_loss = 64.6828, valid batches = 1313\n",
      "Avg test_loss = 56.7740, valid batches = 329\n",
      "time for epoch: 3.5129692554473877 \n",
      "\n",
      "epoch 75/100 \n",
      "-------------------------\n",
      "Avg train_loss = 64.0194, valid batches = 1313\n",
      "Avg test_loss = 85.4765, valid batches = 329\n",
      "time for epoch: 3.6529476642608643 \n",
      "\n",
      "epoch 76/100 \n",
      "-------------------------\n",
      "Avg train_loss = 63.5963, valid batches = 1313\n",
      "Avg test_loss = 57.9892, valid batches = 329\n",
      "time for epoch: 3.7732272148132324 \n",
      "\n",
      "epoch 77/100 \n",
      "-------------------------\n",
      "Avg train_loss = 63.4581, valid batches = 1313\n",
      "Avg test_loss = 55.4539, valid batches = 329\n",
      "time for epoch: 3.6290338039398193 \n",
      "\n",
      "epoch 78/100 \n",
      "-------------------------\n",
      "Avg train_loss = 63.0426, valid batches = 1313\n",
      "Avg test_loss = 56.0790, valid batches = 329\n",
      "time for epoch: 3.6218976974487305 \n",
      "\n",
      "epoch 79/100 \n",
      "-------------------------\n",
      "Avg train_loss = 62.3125, valid batches = 1313\n",
      "Avg test_loss = 55.6068, valid batches = 329\n",
      "time for epoch: 3.5888514518737793 \n",
      "\n",
      "epoch 80/100 \n",
      "-------------------------\n",
      "Avg train_loss = 63.0357, valid batches = 1313\n",
      "Avg test_loss = 66.7905, valid batches = 329\n",
      "time for epoch: 3.616903305053711 \n",
      "\n",
      "epoch 81/100 \n",
      "-------------------------\n",
      "Avg train_loss = 62.6884, valid batches = 1313\n",
      "Avg test_loss = 57.0049, valid batches = 329\n",
      "time for epoch: 3.6221554279327393 \n",
      "\n",
      "epoch 82/100 \n",
      "-------------------------\n",
      "Avg train_loss = 61.9475, valid batches = 1313\n",
      "Avg test_loss = 90.1274, valid batches = 329\n",
      "time for epoch: 3.7139060497283936 \n",
      "\n",
      "epoch 83/100 \n",
      "-------------------------\n",
      "Avg train_loss = 61.7435, valid batches = 1313\n",
      "Avg test_loss = 55.1020, valid batches = 329\n",
      "time for epoch: 3.6326305866241455 \n",
      "\n",
      "epoch 84/100 \n",
      "-------------------------\n",
      "Avg train_loss = 61.4010, valid batches = 1313\n",
      "Avg test_loss = 55.3620, valid batches = 329\n",
      "time for epoch: 3.560100793838501 \n",
      "\n",
      "epoch 85/100 \n",
      "-------------------------\n",
      "Avg train_loss = 61.7191, valid batches = 1313\n",
      "Avg test_loss = 53.1651, valid batches = 329\n",
      "time for epoch: 3.7070393562316895 \n",
      "\n",
      "epoch 86/100 \n",
      "-------------------------\n",
      "Avg train_loss = 60.6209, valid batches = 1313\n",
      "Avg test_loss = 53.2233, valid batches = 329\n",
      "time for epoch: 3.615359306335449 \n",
      "\n",
      "epoch 87/100 \n",
      "-------------------------\n",
      "Avg train_loss = 61.4182, valid batches = 1313\n",
      "Avg test_loss = 52.5325, valid batches = 329\n",
      "time for epoch: 3.601780891418457 \n",
      "\n",
      "epoch 88/100 \n",
      "-------------------------\n",
      "Avg train_loss = 60.9377, valid batches = 1313\n",
      "Avg test_loss = 52.9836, valid batches = 329\n",
      "time for epoch: 3.584886074066162 \n",
      "\n",
      "epoch 89/100 \n",
      "-------------------------\n",
      "Avg train_loss = 60.7138, valid batches = 1313\n",
      "Avg test_loss = 52.4439, valid batches = 329\n",
      "time for epoch: 3.533629894256592 \n",
      "\n",
      "epoch 90/100 \n",
      "-------------------------\n",
      "Avg train_loss = 60.2480, valid batches = 1313\n",
      "Avg test_loss = 52.5395, valid batches = 329\n",
      "time for epoch: 3.5574941635131836 \n",
      "\n",
      "epoch 91/100 \n",
      "-------------------------\n",
      "Avg train_loss = 60.2229, valid batches = 1313\n",
      "Avg test_loss = 51.7691, valid batches = 329\n",
      "time for epoch: 3.624584674835205 \n",
      "\n",
      "epoch 92/100 \n",
      "-------------------------\n",
      "Avg train_loss = 60.5378, valid batches = 1313\n",
      "Avg test_loss = 53.4344, valid batches = 329\n",
      "time for epoch: 3.631767511367798 \n",
      "\n",
      "epoch 93/100 \n",
      "-------------------------\n",
      "Avg train_loss = 59.4867, valid batches = 1313\n",
      "Avg test_loss = 76.4789, valid batches = 329\n",
      "time for epoch: 3.603419303894043 \n",
      "\n",
      "epoch 94/100 \n",
      "-------------------------\n",
      "Avg train_loss = 59.6487, valid batches = 1313\n",
      "Avg test_loss = 51.9397, valid batches = 329\n",
      "time for epoch: 3.752742052078247 \n",
      "\n",
      "epoch 95/100 \n",
      "-------------------------\n",
      "Avg train_loss = 58.8840, valid batches = 1313\n",
      "Avg test_loss = 51.8220, valid batches = 329\n",
      "time for epoch: 3.694857358932495 \n",
      "\n",
      "epoch 96/100 \n",
      "-------------------------\n",
      "Avg train_loss = 58.2480, valid batches = 1313\n",
      "Avg test_loss = 60.4097, valid batches = 329\n",
      "time for epoch: 3.635206460952759 \n",
      "\n",
      "epoch 97/100 \n",
      "-------------------------\n",
      "Avg train_loss = 59.1735, valid batches = 1313\n",
      "Avg test_loss = 62.4937, valid batches = 329\n",
      "time for epoch: 3.6732749938964844 \n",
      "\n",
      "epoch 98/100 \n",
      "-------------------------\n",
      "Avg train_loss = 59.0263, valid batches = 1313\n",
      "Avg test_loss = 56.1461, valid batches = 329\n",
      "time for epoch: 3.735595464706421 \n",
      "\n",
      "epoch 99/100 \n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train_loss = 58.0991, valid batches = 1313\n",
      "Avg test_loss = 50.5255, valid batches = 329\n",
      "time for epoch: 3.7350664138793945 \n",
      "\n",
      "epoch 100/100 \n",
      "-------------------------\n",
      "Avg train_loss = 58.1234, valid batches = 1313\n",
      "Avg test_loss = 53.7297, valid batches = 329\n",
      "time for epoch: 3.627042531967163 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"data/Results/model_pars.pth\"))\n",
    "\n",
    "epochs = 100\n",
    "for index_epoch in range(epochs):\n",
    "    print(f'epoch {index_epoch+1}/{epochs} \\n-------------------------')\n",
    "    t0 = time.time()\n",
    "    train_tmp = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_tmp = test(test_dataloader, model, loss_fn)\n",
    "    tf = time.time()\n",
    "    print(f'time for epoch: {tf-t0} \\n')\n",
    "    train_loss.append(train_tmp)\n",
    "    test_loss.append(test_tmp)\n",
    "    \n",
    "torch.save(model.state_dict(), 'data/Results/model_pars.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([3.9506e-01, 2.0600e+00, 1.5189e+00, 2.3545e+00, 4.1890e+00, 4.5924e+00,\n",
      "        4.7534e+00, 5.5632e+00, 2.1069e+00, 1.6280e+00, 1.5223e+01, 1.0000e+00,\n",
      "        2.8000e+00, 2.5050e+00, 1.8260e+00, 3.9320e+00, 4.3090e+00, 4.7750e+00,\n",
      "        4.5460e+00, 2.8210e+00, 3.8130e+00, 3.0100e+00, 9.0800e-01, 3.0000e+00,\n",
      "        4.0000e+00, 2.2570e+00, 4.8000e-01, 5.0000e-01, 4.1764e+00, 1.0814e+01,\n",
      "        1.1140e+01, 5.8995e+00, 4.4090e-01, 6.3179e-01, 3.8187e-01, 4.8207e-01,\n",
      "        4.2756e-01, 3.2888e-01, 1.2000e+00, 1.5000e+00, 4.0000e-01, 1.8000e+00,\n",
      "        8.0000e-01, 1.4231e+02, 1.0000e+00, 0.0000e+00, 3.0006e-02],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 29.9117,   3.6123],\n",
      "        [200.0000,   2.3309],\n",
      "        [200.0000,   2.6773],\n",
      "        [200.0000,   2.6651],\n",
      "        [200.0000,   3.0953],\n",
      "        [200.0000,   3.1008],\n",
      "        [200.0000,   2.5496],\n",
      "        [200.0000,   2.2066],\n",
      "        [200.0000,   1.4861],\n",
      "        [200.0000,   1.6618],\n",
      "        [200.0000,   1.3897],\n",
      "        [ 40.0000,  12.0000],\n",
      "        [ 10.0000,  12.0000],\n",
      "        [ 10.0000,  12.0000],\n",
      "        [ 10.0000,  12.0000]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 69.9998,   2.2857],\n",
      "        [ 69.9998,   2.4017],\n",
      "        [ 69.9996,   2.2654],\n",
      "        [ 70.0000,   2.3774],\n",
      "        [120.0000,   2.0351],\n",
      "        [120.0000,   1.9438],\n",
      "        [ 70.0000,   2.1568],\n",
      "        [ 70.0000,   2.0442],\n",
      "        [ 69.9938,   1.9025],\n",
      "        [ 49.9931,   1.8606],\n",
      "        [ 69.9969,   1.8642],\n",
      "        [ 99.9990,   2.2961],\n",
      "        [ 80.0000,   3.1416]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 6.5792e-01,  8.8431e-01, -4.0876e-01],\n",
      "        [ 3.5862e-01,  1.0770e+00, -2.8010e+00],\n",
      "        [ 8.8636e-01,  9.2706e-01, -3.6357e-01],\n",
      "        [ 3.7357e-01,  1.0902e+00, -2.8093e+00],\n",
      "        [ 9.9588e-01,  1.1263e+00, -2.8078e+00],\n",
      "        [ 9.9030e-01,  1.1103e+00, -2.8749e+00],\n",
      "        [ 1.0249e+00,  1.2128e+00, -2.4417e+00],\n",
      "        [ 1.6068e-01,  9.9167e-01,  2.5955e+00],\n",
      "        [ 1.0041e+00,  1.1977e+00, -2.4858e+00],\n",
      "        [ 1.0317e+00,  1.1457e+00, -2.2599e+00],\n",
      "        [ 4.1952e-01,  1.1262e+00,  2.3874e+00],\n",
      "        [ 1.0273e+00,  1.1638e+00, -2.3486e+00],\n",
      "        [ 3.8232e-01,  1.1756e+00,  2.4616e+00],\n",
      "        [ 1.0969e+00,  5.0829e-01,  1.8615e+00],\n",
      "        [ 9.6807e-01,  4.6444e-01,  1.8770e+00],\n",
      "        [ 1.2035e+00,  5.3412e-01,  1.8650e+00],\n",
      "        [ 1.1648e+00,  4.9755e-01,  1.8714e+00],\n",
      "        [ 1.1761e+00,  1.2383e+00,  2.7710e-01],\n",
      "        [ 1.1429e+00,  1.0417e+00,  2.7364e+00],\n",
      "        [ 1.0179e+00,  3.0159e+00,  1.5588e+00],\n",
      "        [ 1.0801e+00,  1.0392e+00,  2.5822e+00],\n",
      "        [ 3.5448e-01,  1.0333e+00, -6.3334e-03],\n",
      "        [ 1.8067e-01,  2.0170e+00,  3.1377e+00],\n",
      "        [ 8.8755e-01,  3.0375e+00, -1.4053e-02],\n",
      "        [ 1.0323e+00,  1.9951e+00, -2.2796e-03]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.opt_pars)\n",
    "print(model.bond_type)\n",
    "print(model.angle_type)\n",
    "print(model.tor_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1642\n",
      "1642\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# histogram of amber bond energies\n",
    "\n",
    "bond_energies = []\n",
    "bond_computed_energies =[]\n",
    "for i in range(len(seq_data)):\n",
    "    en1 = seq_data[i]['features'][0,9]   \n",
    "    with torch.no_grad():\n",
    "        en2 = model(seq_data[i]).squeeze()[0]\n",
    "    if torch.isnan(en1) or torch.isnan(en2):\n",
    "        continue\n",
    "    bond_energies.append(en1.item())\n",
    "    bond_computed_energies.append(en2.item())\n",
    "print(len(bond_energies))\n",
    "print(len(bond_computed_energies))\n",
    "\n",
    "hist1 = plt.hist(bond_energies,bins=30,density=True,range=(8,22))\n",
    "hist2 = plt.hist(bond_computed_energies,bins=30,density=True,alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1642\n",
      "1642\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# angle energies\n",
    "\n",
    "angle_energies = []\n",
    "angle_computed_energies =[]\n",
    "for i in range(len(seq_data)):\n",
    "    en1 = seq_data[i]['features'][1,9]   \n",
    "    with torch.no_grad():\n",
    "        en2 = model(seq_data[i]).squeeze()[1]\n",
    "    if torch.isnan(en1) or torch.isnan(en2):\n",
    "        continue\n",
    "    angle_energies.append(en1.item())\n",
    "    angle_computed_energies.append(en2.item())\n",
    "print(len(angle_energies))\n",
    "print(len(angle_computed_energies))\n",
    "\n",
    "hist1 = plt.hist(angle_energies,bins=30,density=True)\n",
    "hist2 = plt.hist(angle_computed_energies,bins=30,density=True,alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1642\n",
      "1642\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAARm0lEQVR4nO3df5BdZ13H8feXjYnOIK20galtwmZMqwQ7Iiwpf4CMRLClaoi0kvUPYpuZCBgFZlCTQTOxMjYBpYPTjhhpSolMWoxEVxsmQCsyOjRki6UhLYWlxGlK7e+pFiZA2q9/3LPlcnt379ns7rk3z75fMzs55znPZr852f3sc59zznMjM5Eklet5/S5AkjS/DHpJKpxBL0mFM+glqXAGvSQVblG/C+h09tln5/DwcL/LkKTTyh133PFoZi7tdmzggn54eJjx8fF+lyFJp5WI+O+pjjl1I0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcQS9JhRu4J2NVnuEtt9Tqd2zHpfNcibQwOaKXpMIZ9JJUOKduNJjGb+jdZ+SK+a9DKoAjekkqnEEvSYVz6kb1OZ0inZYc0UtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVLhaQR8RF0fEvRExERFbuhxfEhE3V8cPRcRwx/HlEfFURLx3juqWJNXUM+gjYgi4DrgEWAWMRsSqjm4bgScycyVwDbCz4/iHgE/PvlxJ0kzVeTJ2NTCRmfcBRMRNwFrg7rY+a4Ht1fY+4NqIiMzMiHgz8C3gO3NVtE5Br6dafaJVKladqZtzgfvb9o9XbV37ZOZJ4EngrIh4PvDHwJ9N9wUiYlNEjEfE+COPPFK3dklSDfO91s124JrMfCoipuyUmbuAXQAjIyM5zzWph6neEWp06MiP7F+97sImypE0S3WC/gFgWdv+eVVbtz7HI2IRcAbwGHARcFlEfAA4E3gmIk5k5rWzLVySVE+doD8MnB8RK2gF+nrgtzv6jAEbgC8ClwG3ZWYCr53sEBHbgacMeUlqVs+gz8yTEbEZOAgMAbsz82hEXAWMZ+YYcD2wJyImgMdp/TKQJA2AWnP0mXkAONDRtq1t+wRweY+/Y/sp1KfTTZe7ezrn9vc+vaapaiThk7GSVDyDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKtx8v/GIVFv7G550LoTWzjc8kWbGoFfjRodu7XcJ0oLi1I0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwrkEgk7Z1v1Tr0cjaXA4opekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOJdAUMv4Dc9ujg65tIFUEkf0klQ4g16SClcr6CPi4oi4NyImImJLl+NLIuLm6vihiBiu2ldHxJ3Vx1ciYt0c1y9J6qFn0EfEEHAdcAmwChiNiFUd3TYCT2TmSuAaYGfV/lVgJDNfDlwM/G1EeF1AkhpUZ0S/GpjIzPsy8/vATcDajj5rgRur7X3AmoiIzPxuZp6s2n8cyLkoWpJUX52gPxe4v23/eNXWtU8V7E8CZwFExEURcRQ4Ary9LfglSQ2Y92mUzDwEvCwiXgrcGBGfzswT7X0iYhOwCWD58uXzXZIWiOEtt9Tue2zHpfNYidRfdUb0DwDL2vbPq9q69qnm4M8AHmvvkJn3AE8BP9/5BTJzV2aOZObI0qVL61cvSeqpTtAfBs6PiBURsRhYD4x19BkDNlTblwG3ZWZWn7MIICJeAvwccGxOKpck1dJz6iYzT0bEZuAgMATszsyjEXEVMJ6ZY8D1wJ6ImAAep/XLAOA1wJaI+AHwDPDOzHx0Pv4hkqTuas3RZ+YB4EBH27a27RPA5V0+bw+wZ5Y1SpJmwXvadfpqW5+nm9GhI+x9ek1DxUiDyyUQJKlwBr0kFc6pmwVk636XH5YWIkf0klQ4g16SCmfQS1LhnKNX0UaHbu3Zx1swVTpH9JJUOINekgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TC+cCUFrzRoVth/OGpO4xc0Vwx0jww6HXacRVOaWacupGkwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXDeR3+aG95yS61+o0Peey4tVI7oJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOJRCkXsZv6N3H95XVAHNEL0mFM+glqXC1gj4iLo6IeyNiIiK2dDm+JCJuro4fiojhqv0NEXFHRByp/nz9HNcvSeqhZ9BHxBBwHXAJsAoYjYhVHd02Ak9k5krgGmBn1f4o8OuZeSGwAdgzV4VLkuqpM6JfDUxk5n2Z+X3gJmBtR5+1wI3V9j5gTUREZv5XZn67aj8K/ERELJmLwiVJ9dQJ+nOB+9v2j1dtXftk5kngSeCsjj5vAb6cmd/r/AIRsSkixiNi/JFHHqlbuySphkYuxkbEy2hN5/xut+OZuSszRzJzZOnSpU2UJEkLRp2gfwBY1rZ/XtXWtU9ELALOAB6r9s8D9gNvy8xvzrZgSdLM1An6w8D5EbEiIhYD64Gxjj5jtC62AlwG3JaZGRFnArcAWzLzP+eoZknSDPR8MjYzT0bEZuAgMATszsyjEXEVMJ6ZY8D1wJ6ImAAep/XLAGAzsBLYFhHbqrY3ZubDc/0PkWZj6/56b55+9boL57kSae7VWgIhMw8ABzratrVtnwAu7/J57wfeP8saJUmz4JOxklQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpXawkEDbbRoVv7XYKkAeaIXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4H5iS5sL4Db37jFwx/3VIXTiil6TCGfSSVDiDXpIKZ9BLUuG8GDughrfc0u8SJBXCEb0kFc6gl6TCGfSSVDjn6KUZ2Lr/SK1+V6+7cJ4rkepzRC9JhTPoJalwBr0kFc45emkedJvL37vvuc9GHNtxaRPlaIFzRC9JhXNEL/VR3SegHflrNhzRS1LhDHpJKpxBL0mFqzVHHxEXAx8GhoCPZuaOjuNLgI8DrwQeA96amcci4ixgH/Aq4GOZuXkui5cWCufyNRs9R/QRMQRcB1wCrAJGI2JVR7eNwBOZuRK4BthZtZ8A/hR475xVLEmakToj+tXARGbeBxARNwFrgbvb+qwFtlfb+4BrIyIy8zvAf0TEyrkrWTo9jQ7d2rPP3qfXNFCJFpo6QX8ucH/b/nHgoqn6ZObJiHgSOAt4tE4REbEJ2ASwfPnyOp+yYNQJB0mazkBcjM3MXZk5kpkjS5cu7Xc5klSUOiP6B4BlbfvnVW3d+hyPiEXAGbQuykqaAad3NB/qjOgPA+dHxIqIWAysB8Y6+owBG6rty4DbMjPnrkxJ0qnqOaKv5tw3Awdp3V65OzOPRsRVwHhmjgHXA3siYgJ4nNYvAwAi4hjwAmBxRLwZeGNm3o0kqRG17qPPzAPAgY62bW3bJ4DLp/jc4VnUJ0mapYG4GCtJmj8GvSQVzqCXpMK5Hn0/jd8w5aHRoee+Q5EknQpH9JJUOINekgpn0EtS4Qx6SSqcQS9JhfOuG+k0M+3CZ+MPt/4cuaKZYnRaMOgb1v6WcN5CKakJTt1IUuEMekkqnEEvSYUz6CWpcAa9JBXOu26kgmzd37qTa+++W6btd2zHpU2UowHhiF6SCmfQS1LhDHpJKpxBL0mFM+glqXDedTNH2tewkaRBYtBLBZp2hUtorXLpCpcLhkEvLUBb9x/pea/9JO+5P/05Ry9JhXNELy1QvaZ39j69pqFKNN8WbNDXvXjqy1YtdHN9o4E/U81bsEE/33peDJMKUOf73FcG/WfQ99BtNNPtm3t0qIlqJGnmvBgrSYUz6CWpcAa9JBXOOXpJjfKOt+Y5opekwhn0klQ4p24kndacCurNoO/gg07S3PKhqv4z6CWp3fgNvfucZks81wr6iLgY+DAwBHw0M3d0HF8CfBx4JfAY8NbMPFYd2wpsBJ4G/iAzD85Z9ZLmTZOvbrt9ra3vq/f1259K95VBdz2DPiKGgOuANwDHgcMRMZaZd7d12wg8kZkrI2I9sBN4a0SsAtYDLwN+GvhcRFyQmU/P9T9kku/0JEk/qs6IfjUwkZn3AUTETcBaoD3o1wLbq+19wLUREVX7TZn5PeBbETFR/X1fnJvyJemHpnsVUvcVwtXrLpyrcp7V7wvGdYL+XOD+tv3jwEVT9cnMkxHxJHBW1X57x+ee2/kFImITsKnafSoi7q1V/dw6G3h0R89ujTsbeLTfRXRhXTM3qLVZV5sdf1Gn15XzUlvsnNWnv2SqAwNxMTYzdwG7+llDRIxn5kg/a+jGumZmUOuCwa3NumZukGvrps4DUw8Ay9r2z6vauvaJiEXAGbQuytb5XEnSPKoT9IeB8yNiRUQspnVxdayjzxiwodq+DLgtM7NqXx8RSyJiBXA+8KW5KV2SVEfPqZtqzn0zcJDW7ZW7M/NoRFwFjGfmGHA9sKe62Po4rV8GVP0+SevC7Ung9+bzjptZ6uvU0TSsa2YGtS4Y3Nqsa+YGubbniNbAW5JUKhc1k6TCGfSSVLgFEfQRsTsiHo6Ir7a1bY+IByLizurjTW3HtkbERETcGxG/2mRdVfvvR8TXIuJoRHyg6bqmqi0ibm47X8ci4s6ma5uirpdHxO1VXeMRsbpqj4j466quuyLiFQ3X9QsR8cWIOBIR/xIRL2g71tT5WhYR/xYRd1ffT++q2l8YEZ+NiG9Uf/5U1d7kOZuqtsur/WciYqTjc+b9vE1T1wern8u7ImJ/RJzZZF2zkpnFfwC/BLwC+Gpb23bgvV36rgK+AiwBVgDfBIYarOuXgc8BS6r9FzVd11S1dRz/K2DbgJyzzwCXVNtvAj7ftv1pIIBXA4ca/h47DLyu2r4S+PM+nK9zgFdU2z8JfL36+h8AtlTtW4CdfThnU9X2UuBngc8DI239Gzlv09T1RmBR1b6z7Zw1+rN5Kh8LYkSfmV+gdTdQHc8u25CZ3wIml21oqq53ADuytWwEmflw03VNUxvQGvUBvwXsbbq2KepKYHK0fAbw7ba6Pp4ttwNnRsQ5DdZ1AfCFavuzwFva6mrqfD2YmV+utv8PuIfW0+lrgRurbjcCb26rralz1rW2zLwnM7s9Hd/IeZumrs9k5smq2+20ngtqrK7ZWBBBP43N1cuw3ZMvXem+5MNzlm2YRxcAr42IQxHx7xHxqgGpq91rgYcy8xvVfr9rezfwwYi4H/hLYOuA1HWUVggAXM4PHx7sS10RMQz8InAIeHFmPlgd+h/gxQNU21Qar22auq6k9cqnL3XN1EIO+r8BfgZ4OfAgramIQbAIeCGtl81/CHyyGkEPklF+OJofBO8A3pOZy4D30HquYxBcCbwzIu6gNQXw/X4VEhHPB/4ReHdm/m/7sWzNP/TtPuvpauunqeqKiPfRei7oE/2qbaYGYq2bfsjMhya3I+LvgH+tdvu9bMNx4FPVD9+XIuIZWos79bsu4NklLn6T1nsPTOp3bRuAd1Xb/wB8dBDqysyv0ZrXJSIuACaXJmy0roj4MVqB9YnM/FTV/FBEnJOZD1ZTM5NThINQ21Qaq22quiLid4BfA9ZUP6ON1nWqFuyIvmPecR0webdEv5dt+CdaF2Qnw2ExrVXy+l3XpF8BvpaZx9va+l3bt4HXVduvByanlMaAt1V3krwaeLJtumLeRcSLqj+fB/wJ8JG2uho5X9WrweuBezLzQ22H2pct2QD8c1t7I+dsmtqm0sh5m6quaL0B0x8Bv5GZ3226rlnp99XgJj5oTTM8CPyA1oh5I7AHOALcRes/6py2/u+jdeX8Xqq7ORqsazHw97R+8XwZeH3TdU1VW9X+MeDtXfr385y9BriD1p0Ph4BXVn2D1pvmfLP6vx5puK530bpj4+vADqon0Rs+X6+hNS1zF3Bn9fEmWsuI30rrl+LngBf24ZxNVdu66hx+D3gIONjkeZumrglac/GTbR9p+v/zVD9cAkGSCrdgp24kaaEw6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1Lh/h/oZpuUvvpfuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# torsion energies\n",
    "\n",
    "torsion_energies = []\n",
    "torsion_computed_energies =[]\n",
    "for i in range(len(seq_data)):\n",
    "    en1 = seq_data[i]['features'][2,9]   \n",
    "    with torch.no_grad():\n",
    "        en2 = model(seq_data[i]).squeeze()[2]\n",
    "    if torch.isnan(en1) or torch.isnan(en2):\n",
    "        continue\n",
    "    torsion_energies.append(en1.item())\n",
    "    torsion_computed_energies.append(en2.item())\n",
    "print(len(torsion_energies))\n",
    "print(len(torsion_computed_energies))\n",
    "\n",
    "hist1 = plt.hist(torsion_energies,bins=30,density=True)\n",
    "hist2 = plt.hist(torsion_computed_energies,bins=30,density=True,alpha=0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
