{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between Adam and SGD optimizers, with different initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../script')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.utils import io\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import LocalEnergyVct as le\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from data_classes import RNASeqDataset, LocalEnergyOpt\n",
    "from sklearn.metrics import r2_score\n",
    "# from my_script import get_target, loss_fn, train, test\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_target(X):\n",
    "    if len(X['features'].shape) == 2:\n",
    "        X['features'] = X['features'].unsqueeze(0)\n",
    "    target = (X['features'][:,0:3,9]).sum(dim=0).squeeze() / X['features'].shape[0]\n",
    "    return target\n",
    "\n",
    "\n",
    "# Functions without gradients in the loss\n",
    "\n",
    "def loss_fn(energy,target):\n",
    "    # batch_size = energy.shape[0]\n",
    "    loss = (energy - target).pow(2).sum()  # / batch_size\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    y_true = torch.zeros((num_batches,3))\n",
    "    y_pred = torch.zeros((num_batches,3))\n",
    "    for i,X in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        target = get_target(X)\n",
    "        y_true[i] = target.reshape(-1,)  # requires drop_last = True\n",
    "        y_pred[i] = pred.reshape(-1,)\n",
    "        loss = loss_fn(pred, target)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    y_true = y_true.detach().numpy()\n",
    "    y_pred = y_pred.detach().numpy()\n",
    "    acc = r2_score(y_true,y_pred)\n",
    "    train_loss /= num_batches\n",
    "    print(f'Avg loss = {train_loss:>0.4f}, batches = {num_batches}')\n",
    "    return train_loss, acc\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    y_true = torch.zeros((num_batches,3))\n",
    "    y_pred = torch.zeros((num_batches,3))\n",
    "    with torch.no_grad():\n",
    "        for i,X in enumerate(dataloader):\n",
    "            pred = model(X)\n",
    "            target = get_target(X)\n",
    "            y_true[i] = target.reshape(-1,) # requires drop_last = True\n",
    "            y_pred[i] = pred.reshape(-1,)\n",
    "            loss = loss_fn(pred, target)\n",
    "            test_loss += loss.item()\n",
    "    y_true = y_true.detach().numpy()\n",
    "    y_pred = y_pred.detach().numpy()\n",
    "    acc = r2_score(y_true,y_pred)\n",
    "    test_loss /= num_batches\n",
    "    print(f'Avg test_loss = {test_loss:>0.4f}, batches = {num_batches}')\n",
    "    return test_loss, acc\n",
    "\n",
    "\n",
    "def KL_divergence(hist1,hist2):\n",
    "    # div(p,q) = sum_x p(x) * log(p(x)/q(x))\n",
    "    # hist[1]: bin limits\n",
    "    # hist[0]: y value\n",
    "    p_x = hist1[0] * np.diff(hist1[1])\n",
    "    q_x = hist2[0] * np.diff(hist2[1])\n",
    "    cond = (q_x>0) & (p_x>0)\n",
    "    div = np.sum( -p_x[cond] * np.log(q_x[cond]/p_x[cond]))\n",
    "    return div\n",
    "\n",
    "\n",
    "def plot_hist(idx_dict,energies):\n",
    "    fig,ax = plt.subplots(1,3,figsize=(18,5))\n",
    "    for i in idx_dict.keys():\n",
    "        hist1 = ax[i].hist(energies['amber'][i], bins=30, density=True, label='Amber')\n",
    "        ax[i].hist(energies['hire'][i], bins=hist1[1], alpha=0.6, density=True, label='HiRE')\n",
    "        ax[i].set_xlabel(idx_dict[i]+' energy', fontsize=15)\n",
    "        ax[i].set_ylabel('Prob. distribution', fontsize=15)\n",
    "        ax[i].set_title(idx_dict[i]+' energy distribution', fontsize=18)\n",
    "        ax[i].legend(fontsize=15)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def compare_energies(dataset,model,plot=False):\n",
    "    \n",
    "    idx_dict = {\n",
    "        0: 'Bonds',\n",
    "        1: 'Angles',\n",
    "        2: 'Torsions'\n",
    "    }\n",
    "    energies = {'amber': [], 'hire': []}\n",
    "    stats = {'amber': [], 'hire': []}\n",
    "    for i in idx_dict.keys():\n",
    "        amber_en = np.array([dataset[j]['features'][i,9].item() for j in range(len(dataset))])\n",
    "        hire_en = np.array([model(dataset[j]).squeeze()[i].item() for j in range(len(dataset))])\n",
    "        energies['amber'].append(amber_en)\n",
    "        energies['hire'].append(hire_en)\n",
    "        stats['amber'].append([amber_en.mean(), amber_en.var()])\n",
    "        stats['hire'].append([hire_en.mean(), hire_en.var()])\n",
    "        print(idx_dict[i]+' energy computed')\n",
    "    stats['amber'] = np.array(stats['amber'])\n",
    "    stats['hire'] = np.array(stats['hire'])\n",
    "    \n",
    "    if plot:\n",
    "        plot_hist(idx_dict,energies)\n",
    "        \n",
    "    return energies,stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define a sort of distances between distributions of energy\n",
    "# Both Kullback-Leibler divergence and total variation distance were implemented\n",
    "\n",
    "def KL_tensor_divergence(hist1,hist2):\n",
    "    # div(p,q) = sum_x p(x) * log(p(x)/q(x))\n",
    "    # hist[1]: bin limits\n",
    "    # hist[0]: y value\n",
    "    p_x = hist1[0] * torch.diff(hist1[1])\n",
    "    q_x = hist2[0] * torch.diff(hist2[1])\n",
    "    cond = (q_x>0) & (p_x>0)\n",
    "    div = torch.sum( -p_x[cond] * torch.log(q_x[cond]/p_x[cond]))\n",
    "    return div\n",
    "\n",
    "\n",
    "def TotVarDist(hist1,hist2):\n",
    "    return torch.sum(torch.abs(hist1[0]-hist2[0]) * torch.diff(hist1[1])).item()\n",
    "\n",
    "\n",
    "def get_amber_hist(dataset):\n",
    "    n_bins = 30\n",
    "    amber_hist_bins = torch.zeros((3,n_bins+1))\n",
    "    amber_hist_values = torch.zeros((3,n_bins))\n",
    "    for i in range(3):\n",
    "        amber_en = torch.tensor([dataset[j]['features'][i,9].item() for j in range(len(dataset))])\n",
    "        amber_hist_values[i], amber_hist_bins[i] = torch.histogram(amber_en, bins=n_bins, density=True)\n",
    "    return (amber_hist_values, amber_hist_bins)\n",
    "\n",
    "\n",
    "def KL_train(dataloader, model, loss_fn, optimizer, amber_hist):\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0\n",
    "    y_pred = torch.zeros((len(dataloader.dataset),3))\n",
    "    KL_score = 0\n",
    "    model.train()    \n",
    "    for i,X in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        target = get_target(X)\n",
    "        y_pred[i: i+dataloader.batch_size] = pred\n",
    "        loss = loss_fn(pred, target)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= num_batches\n",
    "    \n",
    "    for i in range(3):\n",
    "        hist1 = (amber_hist[0][i],amber_hist[1][i])\n",
    "        hist2 = torch.histogram(y_pred[:,i], bins=hist1[1], density=True)\n",
    "        KL_score += TotVarDist(hist1,hist2)  # KL_tensor_divergence(hist1,hist2)\n",
    "        \n",
    "    print(f'Avg loss = {train_loss:>0.4f}, batches = {num_batches}')\n",
    "    return train_loss, KL_score\n",
    "\n",
    "\n",
    "def KL_test(dataloader, model, loss_fn, amber_hist):\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    y_pred = torch.zeros((len(dataloader.dataset),3))\n",
    "    KL_score = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,X in enumerate(dataloader):\n",
    "            pred = model(X)\n",
    "            target = get_target(X)\n",
    "            y_pred[i: i+dataloader.batch_size] = pred\n",
    "            loss = loss_fn(pred, target)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    for i in range(3):\n",
    "        hist1 = (amber_hist[0][i],amber_hist[1][i])\n",
    "        hist2 = torch.histogram(y_pred[:,i], bins=hist1[1], density=True)\n",
    "        KL_score += TotVarDist(hist1,hist2)  # KL_tensor_divergence(hist1,hist2)\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    print(f'Avg test_loss = {test_loss:>0.4f}, batches = {num_batches}')\n",
    "    return test_loss, KL_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# CUDA for Pytorch\n",
    "print(torch.cuda.is_available())\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "device ='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 1,\n",
    "          'shuffle': True,\n",
    "          'drop_last': True,\n",
    "          'num_workers': 0,\n",
    "          'pin_memory': False}\n",
    "\n",
    "# Datasets and Dataloaders\n",
    "seq_data = RNASeqDataset(device=device)\n",
    "print(f'dataset allocated on {device}')\n",
    "tot_length = len(seq_data)\n",
    "test_length = int(0.2 * tot_length)\n",
    "train_set, test_set = random_split(seq_data, [tot_length - test_length, test_length])  #, generator=torch.Generator().manual_seed(42))\n",
    "print(f'Training set: {len(train_set)} elements')\n",
    "print(f'Test set: {len(test_set)} elements')\n",
    "train_dataloader = DataLoader(train_set,**params)\n",
    "test_dataloader = DataLoader(test_set,**params)\n",
    "\n",
    "fixed_pars = pickle.load(open('../data/SeqCSV/fixed_pars.p', 'rb'))\n",
    "opt_pars = pickle.load(open('../data/SeqCSV/pars.p', 'rb'))\n",
    "\n",
    "# set parameters to the same order of magnitude\n",
    "fixed_pars['bond_type'][:,0] /= 100\n",
    "fixed_pars['angle_type'][:,0] /= 100\n",
    "\n",
    "model = LocalEnergyOpt().to(device)\n",
    "# torch.save(model.state_dict(), 'data/Results/initial_values_all1.pth')\n",
    "\n",
    "my_loss = loss_fn  # _with_grad\n",
    "my_train = train  # _with_grad\n",
    "my_test = test  # _with_grad\n",
    "\n",
    "for p in model.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "adam_optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train_loss_adam = []\n",
    "test_loss_adam = []\n",
    "train_acc_adam = []\n",
    "test_acc_adam = []\n",
    "\n",
    "epochs = 150\n",
    "for index_epoch in range(epochs):\n",
    "    print(f'epoch {index_epoch+1}/{epochs} \\n-------------------------')\n",
    "    t0 = time.time()\n",
    "    with io.capture_output() as captured:\n",
    "        train_tmp, train_acc_tmp = my_train(train_dataloader, model, my_loss, adam_optimizer)\n",
    "        test_tmp, test_acc_tmp = my_test(test_dataloader, model, my_loss)\n",
    "    tf = time.time()\n",
    "    print(f'Avg train loss = {train_tmp:.4f}, train accuracy = {train_acc_tmp:.4f}')\n",
    "    print(f'Avg test loss = {test_tmp:.4f}, test accuracy = {test_acc_tmp:.4f}' )\n",
    "    print(f'time for epoch: {tf-t0:.2f} \\n')\n",
    "    train_loss_adam.append(train_tmp)\n",
    "    test_loss_adam.append(test_tmp)\n",
    "    train_acc_adam.append(train_acc_tmp)\n",
    "    test_acc_adam.append(test_acc_tmp)\n",
    "    \n",
    "torch.save(model.state_dict(), '../results/Results_fixedLR/150_b1_e4_Aall1.pth')  # epochs_batchsize_lr_A/SGD + all1/so/~\n",
    "\n",
    "for p in model.parameters():\n",
    "    print(p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model2 = LocalEnergyOpt(fixed_pars,opt_pars,device,set_to_one=True).to(device)\n",
    "lr = 5e-7\n",
    "SGD_optimizer = torch.optim.SGD(model2.parameters(), lr=lr, momentum=0.9)  # consider adding momentum\n",
    "scheduler = scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(SGD_optimizer, 'min', factor=0.5, patience=0, cooldown=1000, threshold=1e-10, verbose=True)\n",
    "train_loss_SGD = []\n",
    "test_loss_SGD = []\n",
    "train_acc_SGD = []\n",
    "test_acc_SGD = []\n",
    "\n",
    "epochs = 150\n",
    "for index_epoch in range(epochs):\n",
    "    print(f'epoch {index_epoch+1}/{epochs} \\n-------------------------')\n",
    "    t0 = time.time()\n",
    "    with io.capture_output() as captured:\n",
    "        train_tmp, train_acc_tmp = my_train(train_dataloader, model2, my_loss, SGD_optimizer)\n",
    "        test_tmp, test_acc_tmp = my_test(test_dataloader, model2, my_loss)\n",
    "    tf = time.time()\n",
    "    print(f'Avg train loss = {train_tmp:.4f}, train accuracy = {train_acc_tmp:.4f}')\n",
    "    print(f'Avg test loss = {test_tmp:.4f}, test accuracy = {test_acc_tmp:.4f}' )\n",
    "    print(f'time for epoch: {tf-t0:.2f} \\n')\n",
    "    train_loss_SGD.append(train_tmp)\n",
    "    test_loss_SGD.append(test_tmp)\n",
    "    train_acc_SGD.append(train_acc_tmp)\n",
    "    test_acc_SGD.append(test_acc_tmp)\n",
    "    \n",
    "torch.save(model.state_dict(), '../results/Results_fixedLR/150_b1_5e7_SGDso.pth')  # epochs_batchsize_lr_A/SGD + all1/so/~\n",
    "\n",
    "for p in model.parameters():\n",
    "    print(p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(14,5))\n",
    "ax[0].semilogy(train_loss_adam, label='Adam')\n",
    "ax[0].semilogy(train_loss_SGD, label='SGD')\n",
    "ax[0].set_xlabel('Epochs', fontsize=15)\n",
    "ax[0].set_ylabel('Loss', fontsize=15)\n",
    "ax[0].legend(fontsize=15)\n",
    "ax[0].set_title('Train loss', fontsize=18)\n",
    "\n",
    "ax[1].semilogy(test_loss_adam, label='Adam')\n",
    "ax[1].semilogy(test_loss_SGD, label='SGD')\n",
    "ax[1].set_xlabel('Epochs', fontsize=15)\n",
    "ax[1].set_ylabel('Loss', fontsize=15)\n",
    "ax[1].legend(fontsize=15)\n",
    "ax[1].set_title('Test loss', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.semilogy(train_loss_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(14,5))\n",
    "ax[0].plot(train_acc_adam, label='Adam')\n",
    "ax[0].plot(train_acc_SGD, label='SGD')\n",
    "ax[0].axhline(y=1, color='black', linestyle='--', linewidth=0.7)\n",
    "ax[0].set_xlabel('Epochs', fontsize=15)\n",
    "ax[0].set_ylabel('R2 score', fontsize=15)\n",
    "ax[0].set_ylim([-2.2,1.5])\n",
    "ax[0].legend(fontsize=15)\n",
    "ax[0].set_title('Train score, batch size = 1', fontsize=18)\n",
    "\n",
    "ax[1].plot(test_acc_adam, label='Adam')\n",
    "ax[1].plot(test_acc_SGD, label='SGD')\n",
    "ax[1].axhline(y=1, color='black', linestyle='--', linewidth=0.7)\n",
    "ax[1].set_xlabel('Epochs', fontsize=15)\n",
    "ax[1].set_ylabel('R2 score', fontsize=15)\n",
    "ax[1].set_ylim([-2.2,1.5])\n",
    "ax[1].legend(fontsize=15)\n",
    "ax[1].set_title('Test score, batch size = 1', fontsize=18)\n",
    "\n",
    "# plt.savefig('Images/Score_400_b16.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adam optimizer analysis: learning rate and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_,stats_adam = compare_energies(seq_data,model,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_,stats_SGD = compare_energies(seq_data,model2,plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Adam results seem better in terms of matching between obtained distributions and original ones. Indeed, one issue to take into consideration is that, if the learning rate is too high (indicatively above $10^{-4}$), it leads to negative couplings, which is obviously unphysical.\n",
    "Limitations to SGD are instead the learning rate, that needs to be small to avoid divergent quantities, and wrong predictions concerning the variance of distributions. Even an increase in the batch size does not modify substantially these behaviours.\n",
    "\n",
    "In particular, the larger discrepancy is observed in the angle energy distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(stats_adam['amber']-stats_adam['hire'])\n",
    "print(stats_SGD['amber']-stats_SGD['hire'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "amber_hist = get_amber_hist(seq_data)\n",
    "# print(amber_hist[0][0], amber_hist[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"../results/Results_fixedLR/initial_values_sameorder.pth\"))\n",
    "lr = 1e-4\n",
    "adam_optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "my_train = KL_train\n",
    "my_test = KL_test\n",
    "epochs = 200\n",
    "\n",
    "train_loss_adam = []\n",
    "test_loss_adam = []\n",
    "train_acc_adam = []\n",
    "test_acc_adam = []\n",
    "\n",
    "for index_epoch in range(epochs):\n",
    "    print(f'epoch {index_epoch+1}/{epochs} \\n-------------------------')\n",
    "    t0 = time.time()\n",
    "    with io.capture_output() as captured:\n",
    "        train_tmp, train_acc_tmp = my_train(train_dataloader, model, my_loss, adam_optimizer, amber_hist)\n",
    "        test_tmp, test_acc_tmp = my_test(test_dataloader, model, my_loss, amber_hist)\n",
    "    tf = time.time()\n",
    "    print(f'Avg train loss = {train_tmp:.4f}, train score = {train_acc_tmp:.4f}')\n",
    "    print(f'Avg test loss = {test_tmp:.4f}, test score = {test_acc_tmp:.4f}' )\n",
    "    print(f'time for epoch: {tf-t0:.2f} \\n')\n",
    "    train_loss_adam.append(train_tmp)\n",
    "    test_loss_adam.append(test_tmp)\n",
    "    train_acc_adam.append(train_acc_tmp)\n",
    "    test_acc_adam.append(test_acc_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(train_acc_adam, label='Train')\n",
    "plt.plot(test_acc_adam, label='Test')\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=0.7)\n",
    "plt.xlabel('Epochs', fontsize=15)\n",
    "plt.ylabel('TVD score', fontsize=15)\n",
    "plt.ylim([-0.5,3])\n",
    "plt.legend(fontsize=15)\n",
    "plt.title('TVD score, batch size = 1', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# try same hyperparameters, but set initial values to 1\n",
    "\n",
    "\n",
    "model2 = LocalEnergyOpt(fixed_pars,opt_pars,device,set_to_one=True).to(device)\n",
    "for p in model2.parameters():\n",
    "    print(p)\n",
    "lr = 1e-4\n",
    "\n",
    "adam_optimizer = torch.optim.Adam(model2.parameters(), lr=lr)\n",
    "my_train = KL_train\n",
    "my_test = KL_test\n",
    "epochs = 100\n",
    "\n",
    "train_loss_adam2 = []\n",
    "test_loss_adam2 = []\n",
    "train_acc_adam2 = []\n",
    "test_acc_adam2 = []\n",
    "\n",
    "for index_epoch in range(epochs):\n",
    "    print(f'epoch {index_epoch+1}/{epochs} \\n-------------------------')\n",
    "    t0 = time.time()\n",
    "    with io.capture_output() as captured:\n",
    "        train_tmp, train_acc_tmp = my_train(train_dataloader, model2, my_loss, adam_optimizer, amber_hist)\n",
    "        test_tmp, test_acc_tmp = my_test(test_dataloader, model2, my_loss, amber_hist)\n",
    "    tf = time.time()\n",
    "    print(f'Avg train loss = {train_tmp:.4f}, train score = {train_acc_tmp:.4f}')\n",
    "    print(f'Avg test loss = {test_tmp:.4f}, test score = {test_acc_tmp:.4f}' )\n",
    "    print(f'time for epoch: {tf-t0:.2f} \\n')\n",
    "    train_loss_adam2.append(train_tmp)\n",
    "    test_loss_adam2.append(test_tmp)\n",
    "    train_acc_adam2.append(train_acc_tmp)\n",
    "    test_acc_adam2.append(test_acc_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(train_acc_adam2, label='Train')\n",
    "plt.plot(test_acc_adam2, label='Test')\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=0.7)\n",
    "plt.xlabel('Epochs', fontsize=15)\n",
    "plt.ylabel('TVD score', fontsize=15)\n",
    "plt.ylim([-0.5,3])\n",
    "plt.legend(fontsize=15)\n",
    "plt.title('TVD score, batch size = 1', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for p in model2.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_,stats_adam_so = compare_energies(seq_data,model,plot=True)\n",
    "_,stats_adam_all1 = compare_energies(seq_data,model2,plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Starting with same parameters for Adam, but with all initial values for the coupling constants set to 1 leads to a similar score, but unfortunately one of the parameters becomes negative. Nevertheless, it is interesting to notice that, even if the value are different, some patterns can be found, especially in the relations between the coefficients.\n",
    "For example, the first bond coupling tends to go to a lower value than the others, independently of the initialization, and similar patterns are obtained also for angle parameters, especially the first ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pars_so = [p.data for p in model.parameters()]\n",
    "pars_1 = [p.data for p in model2.parameters()]\n",
    "\n",
    "plt.plot(pars_so[0],'o')\n",
    "plt.plot(pars_1[0],'o')\n",
    "plt.ylim(0,7)\n",
    "plt.show()\n",
    "print(pars_so[0][0])\n",
    "print(pars_1[0][0])\n",
    "\n",
    "# x and y coordinates are coupling constant and equilibrium value, respectively\n",
    "\n",
    "plt.scatter(pars_so[1][:,0]*pars_so[0][0],pars_so[1][:,1],label='Same Order')\n",
    "plt.scatter(pars_1[1][:,0]*pars_1[0][0],pars_1[1][:,1],label='All 1')\n",
    "for i in range(len(pars_so[1])):\n",
    "    plt.annotate(i+1, (pars_so[1][i,0]*pars_so[0][0],pars_so[1][i,1]))\n",
    "    plt.annotate(i+1, (pars_1[1][i,0]*pars_1[0][0],pars_1[1][i,1]))\n",
    "plt.ylim(0,4)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(pars_so[2][:,0]*pars_so[0][-1],pars_so[2][:,1],label='Same Order')\n",
    "plt.scatter(pars_1[2][:,0]*pars_1[0][-1],pars_1[2][:,1],label='All 1')\n",
    "for i in range(len(pars_so[2])):\n",
    "    plt.annotate(i+1, (pars_so[2][i,0]*pars_so[0][-1],pars_so[2][i,1]))\n",
    "    plt.annotate(i+1, (pars_1[2][i,0]*pars_1[0][-1],pars_1[2][i,1]))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(pars_so[3][:,0],pars_so[3][:,1],label='Same Order')\n",
    "plt.scatter(pars_1[3][:,0],pars_1[3][:,1],label='All 1')\n",
    "for i in range(len(pars_so[3])):\n",
    "    plt.annotate(i+1, (pars_so[3][i,0],pars_so[3][i,1]))\n",
    "    plt.annotate(i+1, (pars_1[3][i,0],pars_1[3][i,1]))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "amber_hist = get_amber_hist(seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(train_acc_adam, label='Train')\n",
    "plt.plot(test_acc_adam, label='Test')\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=0.7)\n",
    "plt.xlabel('Epochs', fontsize=15)\n",
    "plt.ylabel('TVD score', fontsize=15)\n",
    "plt.ylim([-0.5,3])\n",
    "plt.legend(fontsize=15)\n",
    "plt.title('TVD score, batch size = 1', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# try same hyperparameters, but set initial values to 1\n",
    "\n",
    "\n",
    "model2 = LocalEnergyOpt(fixed_pars,opt_pars,device,set_to_one=True).to(device)\n",
    "for p in model2.parameters():\n",
    "    print(p)\n",
    "lr = 1e-4\n",
    "\n",
    "adam_optimizer = torch.optim.Adam(model2.parameters(), lr=lr)\n",
    "my_train = KL_train\n",
    "my_test = KL_test\n",
    "epochs = 200\n",
    "\n",
    "train_loss_adam2 = []\n",
    "test_loss_adam2 = []\n",
    "train_acc_adam2 = []\n",
    "test_acc_adam2 = []\n",
    "\n",
    "for index_epoch in range(epochs):\n",
    "    print(f'epoch {index_epoch+1}/{epochs} \\n-------------------------')\n",
    "    t0 = time.time()\n",
    "    with io.capture_output() as captured:\n",
    "        train_tmp, train_acc_tmp = my_train(train_dataloader, model2, my_loss, adam_optimizer, amber_hist)\n",
    "        test_tmp, test_acc_tmp = my_test(test_dataloader, model2, my_loss, amber_hist)\n",
    "    tf = time.time()\n",
    "    print(f'Avg train loss = {train_tmp:.4f}, train score = {train_acc_tmp:.4f}')\n",
    "    print(f'Avg test loss = {test_tmp:.4f}, test score = {test_acc_tmp:.4f}' )\n",
    "    print(f'time for epoch: {tf-t0:.2f} \\n')\n",
    "    train_loss_adam2.append(train_tmp)\n",
    "    test_loss_adam2.append(test_tmp)\n",
    "    train_acc_adam2.append(train_acc_tmp)\n",
    "    test_acc_adam2.append(test_acc_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(train_acc_adam2, label='Train')\n",
    "plt.plot(test_acc_adam2, label='Test')\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=0.7)\n",
    "plt.xlabel('Epochs', fontsize=15)\n",
    "plt.ylabel('TVD score', fontsize=15)\n",
    "plt.ylim([-0.5,3])\n",
    "plt.legend(fontsize=15)\n",
    "plt.title('TVD score, batch size = 1', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for p in model2.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Starting with same parameters for Adam, but with all initial values for the coupling constants set to 1 leads to a similar score, but unfortunately one of the parameters becomes negative. Nevertheless, it is interesting to notice that, even if the value are different, some patterns can be found, especially in the relations between the coefficients.\n",
    "For example, the first bond coupling tends to go to a lower value than the others, independently of the initialization, and similar patterns are obtained also for angle parameters, especially the first ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelso = LocalEnergyOpt()\n",
    "modelso.load_state_dict(torch.load('../results/Results_fixedLR/150_b1_e4_Aso.pth'))\n",
    "model1 = LocalEnergyOpt()\n",
    "model1.load_state_dict(torch.load('../results/Results_fixedLR/150_b1_e4_Aall1.pth'))\n",
    "modelin = LocalEnergyOpt()\n",
    "modelin.load_state_dict(torch.load(\"../results/Results_fixedLR/initial_values_sameorder.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_bonds = torch.cat((modelin.bond_type.data, model1.bond_type.data, modelso.bond_type.data), dim=1)\n",
    "print(all_bonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_angles = torch.cat((modelin.angle_type.data, model1.angle_type.data, modelso.angle_type.data), dim=1)\n",
    "print(all_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_tors = torch.cat((modelin.tor_type.data, model1.tor_type.data, modelso.tor_type.data), dim=1)\n",
    "torch.set_printoptions(linewidth=120, sci_mode=False)\n",
    "print(all_tors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
